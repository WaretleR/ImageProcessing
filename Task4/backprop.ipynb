{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "## Метод обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В этом задание вы:\n",
    "* познакомитесь с методом обратного распространения ошибки \n",
    "* реализуете прямой проход и обратный проход в нейросети\n",
    "* реальзуете стохастический градиентный спуск с моментов\n",
    "* обучите нейросеть для классификации рукописных цифр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Прямой и обратный проход нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Теоретическая часть\n",
    "\n",
    "<p>\n",
    "Метод обратного распространения ошибки — это метод обучения многослойной нейрононной сети, который впервые был открыт двумя независимыми группами исследователей в 1974 г. Этот метод определяет алгоритм эффективного вычисления градиентов параметров нейронной сети, что позволяется применить метод градиентного спуска в задачи минимизации функционала ошибки. \n",
    "</p>\n",
    "\n",
    "Давайте рассморим M-слойную полносвязную нейронную сеть.\n",
    "\n",
    "<img src=\"network.png\" width=\"800\">\n",
    "\n",
    "На рисунке верхний индекс всегда используется для обозначения номера слоя нейросети. Рассмотрим некоторый n-ый слой, который назовем текущим. Данный слой на вход принимает\n",
    "$N^{(n-1)}$, \n",
    "признаков \n",
    "$y^{(n - 1)}_i$, $i=\\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n-1)}}$, \n",
    "которые являются значения выходов нейронов предыдущего слоя, и \n",
    "$y^{(n-1)}_0=1$\n",
    "(смещение или bias нейрона — константа, которая рассматривается как вход нейрона для упрощения записи дальнейших вычислений). Вес нейрона, связывающий $i$'ый нейрон предыдущего слоя с $j$'ым нейроном текущего, обозначен $w^{(n)}_{ij}$. \n",
    "За \n",
    "$z_j^{(n)}=\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$, \n",
    "обозначена линейная комбинация входов и весов.\n",
    "$\\sigma^{(n)}_j$ \n",
    "— функция активации j'ого нейрона(так как функции активации нейронов в общем случае могут быть различны), а \n",
    "$y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)}) = \\sigma_j^{(n)}(\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}) $ \n",
    "— значение функции активации или выход j'ого нейрона. \n",
    "\n",
    "<img src=\"layer.png\" width=\"800\">\n",
    "\n",
    "Резюмируем обозначения:\n",
    "* $M$ - колличество слоев\n",
    "* $N^{(n)}$ - колличество нейронов в $n$-ом слое\n",
    "* $\\{ w_{ij}^{(n)}\\}$ - веса нейронов $n$-ого слоя\n",
    "* $z_j^{(n)} = \\sum_{i=0}^{N^{(n - 1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$\n",
    "* $\\sigma_j^{(n)}$ - функция активации $j$-ого нейрона $n$-ого слоя\n",
    "* $y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)})$ - выход $j$-ого нейрона $n$-ого слоя \n",
    "\n",
    "За $E(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})$  обозначим некоторую диффенцируюмую функцию ошибки. Здесь  <br/>\n",
    "$\\boldsymbol{\\widehat{y}} = (y_1, ..., y_{N^{(M)}})$ - значение целевой переменной,  \n",
    "$\\boldsymbol{y} = (y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}))$ - выход нейросети. \n",
    "\n",
    "Давайте попробуем оценить сложность вычисление частной производной функции ошибки $E$. Предположим, что сложность вычисления частной производной функции в точке приблизительно равна сложности вычисления самой функции в точке. Пусть нейросеть имеет M полносвязных слоем по N нейронов в каждом слое. Тогда:\n",
    "* $O(N)$ - cложность вычисления одного выхода одного слоя (перемножение N весов на N входов)\n",
    "* $O(N^2)$ - cложность вычисления всех выходов одного слоя\n",
    "* $O(M * N^2)$ - cложность вычисления функции ошибки (последовательно вычисляем выходы M слоев)\n",
    "* $O(M^2 * N^4)$ - cложность частных производных функции ошибки по всем весам (всего  $O(M * N^2)$ весов)\n",
    "\n",
    "Получается для нейросети, состоящей из одного слоя с 1000 нейронами, сложность вычисления градиента должна быть равна $O(10^{12})$. А это уже невероятно много.\n",
    "\n",
    "Идея метода эффективного расчета градиентов заключается в том, чтобы при прямом проходе нейросети сохранить некоторые вычисленные значения, которые потом позволят быстро находить градиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Будем вычислять частные производные функции ошибки от последних слоев в первым.\n",
    "Помимо частных производных \n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$$ \n",
    "будем вычислять значения производных \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial y_i^{(n)}} E \\quad \\mathrm{и} \\quad \\frac{\\mathrm{\\partial}}{\\partial z_i^{(n)}} E$$ вычисление которых является одним из ключевых моментов в алгоритме обратном распространения ошибки. \n",
    "\n",
    "Полезно посмотреть как аналитически выглядит вычисления выхода нейросети \n",
    "$\\boldsymbol{\\widehat{y}}=(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)})$.:\n",
    "\n",
    "$$ \\widehat{y_j}=y_j^{(M)}=\\sigma_j^{(M)}(\\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})})$$\n",
    "\n",
    "В случае однослойной нейросети, получим: \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(1)}(\\sum_{i=0}^{N^{(0)}}{w_{ij}^{(1)}x_j}),$$\n",
    "\n",
    "двухслойной - \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(2)}(\\sum_{i=0}^{N^{(1)}}{w_{ij}^{(2)}\\sigma_i^{(1)}(\\sum_{k=0}^{N^{(0)}}{w_{ki}^{(1)}x_k})}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вычисление градиента начнем с выходного слоя.** \n",
    "\n",
    "Вспомним как вычисляется выход:\n",
    "\n",
    "$$ y_j^{(M)}=\\sigma_j^{(M)}(z_j^{(M)})$$\n",
    "$$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$$\n",
    "\n",
    "Будем последовательно вычислять:\n",
    "\n",
    "1. $\\boldsymbol{\\frac{\\partial E}{\\partial y_j^{(M)}}}.$\n",
    "Так как функция E нам известна, то мы можем вычислить частные производные этой функции по переменным \n",
    "$y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}. $ \n",
    "Например, если\n",
    "$$E(\\boldsymbol{\\widehat{y}},  \\boldsymbol{y}^{(M)}) = \\frac{1}{2} \\| \\boldsymbol{\\widehat{y}} - \\boldsymbol{y}^{(M)} \\|_2^2 = \\frac{1}{2} \\sum_{j=1}^{N^{(M)}}{( \\widehat{y_j} - y_j^{(M)}) ^ 2},$$ \n",
    "то\n",
    "$$\\frac{\\partial E}{\\partial y_j^{(M)}} = y_j^{(M)} - \\widehat{y_j}$$\n",
    "Заметим, что в этом случае частная производная $E$ по $y_j^{(M)}$ равна ошибки на объекте $x_i$.\n",
    "\n",
    "2. $\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}) = E(y_1^{(M)}(z_1^{(M)}), ..., y_{N^{(M)}}^{(M)}(z_{N^{(M)}}^{(M)}))$.\n",
    "Вычислим \n",
    "$\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}$ \n",
    "применив правило вычисление производной сложной функции \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial z_j^{(M)}} E = \\frac{\\partial E}{\\partial y_j^{(M)}} \\frac{\\partial y_j^{(M)}}{\\partial z_j^{(M)}} =  \\frac{\\partial E}{\\partial y_j^{(M)}} (\\sigma^{(M)}_j)' $$\n",
    "Важно, что $(\\sigma^{(M)}_j)'$ берется в точке \n",
    "$$z_j^{(M)}= \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})}.$$ \n",
    "Заметим что при прямом проходе, мы  вычисляем значение $z_j^{(M)}$, теперь дополнительно при прямом проходе будет сохранять это значение. Тогда вычисление \n",
    "$\\frac{\\mathrm{\\partial}}{\\partial z_i^{(M)}} E$ \n",
    "представляет собой вычисление значение  $(\\sigma^{(M)}_j)'$ в уже известной точке и перемножение двух чисел.\n",
    "\n",
    "3. $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(z_1^{(M)}, z_2^{(M)}, ..., z_{N^{(M)}}^{(M)})$. \n",
    "Вспомним, что \n",
    "$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$. \n",
    "Вес $w_{ij}^{(M)}$ входит только в одну сумму $z_j^{(M)}$. Тогда:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}^{(M)}}= \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial z_j^{(M)}}{\\partial w_{ij}^{(M)}} =   \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial(\\sum_{k=0}^{N^{(M - 1)}}{w_{kj}^{(M)}y_k^{(M - 1)}}) }{\\partial w_{ij}^{(M)}} = \\frac{\\partial E}{\\partial z_j^{(M)}} y_i^{(M - 1)} $$\n",
    "\n",
    "Таким образом, мы  за 3 шага вычислили $\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$ для последнего выходного слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вычисление градиента произвольного внутренного слоя**\n",
    "\n",
    "Рассмотрим произвольный внутренний слой n. Вспомним что выходы этого слоя $y_i^{(n)}$ связаны с $z_j^{(n + 1)}$ следующего слоя соотношением:\n",
    "$$z_j^{(n + 1)}=\\sum_{i=0}^{N_{n}}{w_{ij}^{(n + 1)}y_i^{(n)}}, \\quad  j= \\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n+1)}}$$\n",
    "Можно сказать, что \n",
    "$E = E(z_1^{(n + 1)}, z_2^{(n + 1)},..., z_{N^{(n+1)}}^{(n + 1)})$. \n",
    "А производные \n",
    "$\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}$ \n",
    "были посчитаны на предыдущем шаге.\n",
    "\n",
    "Тогда для \n",
    "$\\boldsymbol{\\frac{\\partial E}{\\partial y_i^{(n)}}}$ \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial z_{j}^{(n + 1)}}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial (\\sum_{k=0}^{N_{n}}{w_{kj}^{(n + 1)}y_k^{(n)}})}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} $$\n",
    "\n",
    "Эту величину, по аналогии с последним слоем будем называть ошибкой сети на скрытом слое. Заметим, что \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} =  \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial y_{j}^{(n + 1)}}}(\\sigma^{(n + 1)}_j)'  w_{ij}^{(n + 1)}.$$\n",
    "Таким образом, мы вычисляем ошибку текущего слоя через ошибку предыдущего, распространяя ее \"задом наперед\". Отсюда и название алгоритма — обратное распространение ошибок.\n",
    "\n",
    "Вычисление\n",
    "$\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_i^{(n)}}}$ и $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(n)}}}$ выполняется абсолютно аналогично последнему слою.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы умеем последовательно вычислять частные производные по всем весам от последнего слоя к нейросети к первому. Заметим что все произодные можно быстро вычислять матрично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части вам предстоит научиться:\n",
    "* вычислять производные среднеквадратичной функции ошибки и категориальной кросс энтропии\n",
    "* выполнять прямой и обратный проход неросети, состоящей из полносвязных слоей и функции антивации ReLu и Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.** Вычисление производных функций ошибок\n",
    "\n",
    "Вам дан интерфейс класса `Loss`, вам нужно реализовать вычисление значение функции и ее градиента для среднеквадратичной ошибки $MSE$, а также для категориальной кросс-энтропии $H$. \n",
    "\n",
    "Пусть $\\widehat{y_i}$ - истинное значение функции, $y_i$ - предсказанное, тогда:\n",
    "$$MSE(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=\\frac{1}{d}\\sum_{i=1}^{d}{( \\widehat{y_i} - y_i)^2}$$\n",
    "$$H(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=-\\frac{1}{d}\\sum_{i=1}^{d}{\\widehat{y_i}\\log({y_i})}$$\n",
    "\n",
    "Для численной устойчивости вам предлагается также реализовать градиент связки `softax + crossentropy`(в функции `gradient_with_sofmax`). Преобразование $softmax$ над вектором $x=(x_1, ..., x_d)$ можно записать как \n",
    "$$y_i = \\frac{e^{x_i}}{\\sum_{j=0}^{d}{e^{x_j}}}$$\n",
    "Функция `gradient_with_sofmax` должна возвращать \n",
    "$(\\frac{\\partial E}{\\partial x_1}, ..., \\frac{\\partial E}{\\partial x_d})$\n",
    "(Вам необходимо выполнить аналитические преобразования над $\\frac{\\partial E}{\\partial x_i}$ так, чтобы аналичически он зависел только от $\\boldsymbol{\\widehat{y}}$ и $\\boldsymbol{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import base64\n",
    "import copy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: loss\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: np.array(d), gradient loss to y_pred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        return (-2) * (y_true - y_pred) / y_true.shape[0]\n",
    "\n",
    "\n",
    "class CategoricalCrossentropy(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return (-1) * np.mean(y_true * np.log(y_pred))\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        return (-1) * (y_true / y_pred)\n",
    "\n",
    "    def gradient_with_softmax(self, y_true, y_pred):\n",
    "        y_exp = np.exp(y_pred)\n",
    "        result = np.zeros(y_true.shape)\n",
    "        for i in range(y_true.shape[0]):\n",
    "            result[i] = y_pred[i] - y_true[i]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедитесь в правильности работы ваших функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}.\\n\\ty_true:{},\\n\\ty_pred:{},\\n\\tactual output:{}\\n\\tdesired output:{}\"\n",
    "\n",
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    for i, (actual_value, desired_value) in enumerate(zip(actual_values, desired_values)):\n",
    "        msg = ERROR_MSG.format(i, Y_TRUE[i], Y_PRED[i], actual_value, desired_value)\n",
    "        np.testing.assert_almost_equal(actual_value, desired_value, err_msg=msg, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "\n",
    "SHAPE = (10, 5)\n",
    "Y_TRUE = np.rint(np.random.random(size=SHAPE))\n",
    "Y_PRED = np.random.random(size=SHAPE)\n",
    "\n",
    "MSE_ERRORS = decode_answer(\n",
    "    b'I0+rUDZfsj+65BmGzuvTP+ITdyZtp9E/jovoXZbOxj+tM4rNGfjRP8WqZPINQ7o/qkWFqWJrvj/YygOu3+7iP2xXHf8FLtc/nh8u'\n",
    "    b'JeH90z8=', shape=SHAPE[0]\n",
    ")\n",
    "\n",
    "MSE_GRADIENTS = decode_answer(\n",
    "    b'bbbcKhRtpD9tOdGaZi27v2ZYmkJFG3s//n322ZiNyr8m6+LE9aWjP6AI7iZLdsk/C3BaoCJ51T/NU4CxsQalvyCAv50cwdI/Jnla'\n",
    "    b'YuW0uL+ge/FUpbTMP8oHwWfBI8g/078UJnJrrD+agsQdii7Xv/P1Wpvcppe/ALOZhal2qz/J0z1DWiXQPy69kl9AdMK/KyUC2VwB'\n",
    "    b'zb+TKsfV3+qvP61ZjQQ/I6y/861aiGl80T9aw6ZZnY2hP05TbFQuN8a/pf7fNLXD1b91mBtGF3LIP2BQLEj8766/QK1QkZqzrD8A'\n",
    "    b'Z3EKfd/APyILN4pDvMK/IkWU2OF8w7/dKXoh5znEP2Op3oXyxcC/bf/5e0B1s7+ti6ZZedDDP8APmQhD5Ms/m5mMtC1R2T9T4D8Z'\n",
    "    b'LzfZPy+LSRFEHtC/gzVVrZfSzD+G39szoJO5P2acoFCBSoO/d+LLLFDn1D+yZblH9XbYv5OyW97C+8Q/nCtIKLzV2L9dGTmp/Em1'\n",
    "    b'P7SuSAYBJtA//Z/XUtyLwb+9hvZzq4e5Pw==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTROPY_ERRORS = decode_answer(\n",
    "    b'Lk/El56cyj/WBoxO/q6zP0unis259t4/pUfrxKNr0D96LACShLzgPwpVpmwNtr8/yNI87LnRyz93hYk3LG3JP09lbzfFF+Q/stF9'\n",
    "    b'Vtkm6T8=', shape=(SHAPE[0])\n",
    ")\n",
    "CROSS_ENTROPY_GRADIENTS = decode_answer(\n",
    "    b'AAAAAAAAAICQTFUt2sf1vwAAAAAAAACAmWpZOGWeAMAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAICCQnznotTxvwAAAAAAAACAXyHI'\n",
    "    b'8ogW9b8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAID9kuL7NywlwN9kei0D+/C/AAAAAAAAAIAAAAAAAAAAgNm59BtSBPm/wPXRt3J0'\n",
    "    b'AsAAAAAAAAAAgDhJMLFijPK/AAAAAAAAAIAAAAAAAAAAgIi4e0BwQ/y/9c0zp6WyGsAAAAAAAAAAgHD+3KnZ2PK/AAAAAAAAAIAA'\n",
    "    b'AAAAAAAAgFqtaQ7QO/m/QoDlrBTV+b8AAAAAAAAAgAhQB8ahy/e/DnZxuenA878AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAgHRxm/JtmQXAAAAAAAAAAIAAAAAAAAAAgKLsIAbIYvC/AAAAAAAAAIDiZmdoeIw2wAAAAAAAAACA5l+CC9q6QMAAAAAAAAAA'\n",
    "    b'gAAAAAAAAACAtRNb8pFX+L8AAAAAAAAAgA==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX = decode_answer(\n",
    "    b'COSTNVmIuT/kw8IgYPzQv0B3oEkL8ZA/vw46iH+Y4L/wpRs2c4+4P8iKqfDd098/DgxxSGvX6j/AaOAdXki6vyhgL8Vjcec/cBfx'\n",
    "    b'uh7izr9E7RZV5/DhP7xJscGxLN4/5PfMVyfDwT9BozWlLPrsv3CzMcKTkK2/4A+A8ykqwT+7SA3UsC7kP3psd3dQEde/O1ehB9og'\n",
    "    b'4r+cepzly/LDPwxY2GIHlsG/cFlx6oPb5T8wdBCwBPG1PyJoh+n5xNu/Tv4XgqI067+SfqIXnY7ePzyyG639VcO/SGzSmkDwwT/A'\n",
    "    b'wA1NXBfVP+rNxGxUa9e/ala5Thpc2L9UtNjpYEjZP7xTVidv99S/SH/4mpBSyL+YLhCwl8TYP9ipX+WpbuE/AsCvIXml7z9o2I/f'\n",
    "    b'+oTvP/vtmxXVJeS/ckFVzJ4D4j9o19JAiPjPP4DDyKQhHZi/Fdv+NyQh6j8ev6eZspTuvzif8pWzOto/gzZaMisL77+0X4fTe5zK'\n",
    "    b'P2Ha2keBL+Q//IeNZ9Pu1b9sKPRQlunPPw==', shape=SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_errors = [MeanSquaredError()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(MSE_ERRORS, mse_errors)\n",
    "\n",
    "mse_gradients = [MeanSquaredError().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(MSE_GRADIENTS, mse_gradients)\n",
    "\n",
    "cross_entropy_errors = [CategoricalCrossentropy()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_ERRORS, cross_entropy_errors)\n",
    "\n",
    "cross_entropy_gradients = [CategoricalCrossentropy().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_GRADIENTS, cross_entropy_gradients)\n",
    "\n",
    "cross_entropy_grad_softmax = [CategoricalCrossentropy().gradient_with_softmax(y_true, y_pred) \n",
    "                              for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX, cross_entropy_grad_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.** Выполнение прямого и обратного прохода нейросети\n",
    "\n",
    "В этой части задания вам будет необходимо реализовать forward и backward проходы для 3-х типов слоев:\n",
    "* `Linear`, выполняет линейную комбинацию входов с весами $y_j=\\sum_{i=0}^{N}{w_{ij}x_i}, j=\\overline{1 {\\ldotp \\ldotp}M}$\n",
    "* `ReLu`, нелинейная активация $y_j^{(n)}=\\max(x_j), j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "* `Softmax`, $y_j = \\frac{e^{x_j}}{\\sum_{i=0}^{d}{e^{x_i}}}, j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "\n",
    "Вам дан шаблон нейросетевой модели, которая умееет последовательно добавлять слои друг за другом. Сейчас вам стоит обратит внимание только на функции `__init__`, `add`, `forward` и `backward`. Пока при инициализации модели параметр `optimizer` оставляйте равным None. Также вам дан абстракный класс `Layer`, который предоставляет интерфейс одного слоя нейросети. Вам необходимо реализовать функции `_forward` и `_backward`. Все вычисления необходимо делать матрично.\n",
    "\n",
    "Замечание:\n",
    "    обратите внимание на функцию `_build` слоя `Linear`, веса соответвущие bias'ам добавляются последним столбцом.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, loss=None, optimizer=None):\n",
    "        self._layers = []\n",
    "        self._loss = loss\n",
    "        self._optimizer = optimizer\n",
    "        self._outputs = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not self._layers:\n",
    "            layer.build(optimizer=self._optimizer)\n",
    "        else:\n",
    "            layer.build(self._layers[-1], optimizer=self._optimizer)\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs\n",
    "        for layer in self._layers:\n",
    "            outputs = layer.forward(outputs)\n",
    "        self._outputs = outputs\n",
    "        return self._outputs\n",
    "\n",
    "    def backward(self, outputs, use_gradient_softmax_with_loss=False):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "\n",
    "        if use_gradient_softmax_with_loss:\n",
    "            grad_outputs = [self._loss.gradient_with_softmax(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers[:-1]\n",
    "        else:\n",
    "            grad_outputs = [self._loss.gradient(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers\n",
    "\n",
    "        grad_outputs = np.array(grad_outputs)\n",
    "        for layer in backward_layers[::-1]:\n",
    "            grad_outputs = layer.backward(grad_outputs)\n",
    "\n",
    "    def update_weights(self, x_batch, y_batch, use_gradient_softmax_with_loss=False):\n",
    "        if self._optimizer is None:\n",
    "            raise ValueError(\"Optimizer is not defined\")\n",
    "        self.forward(x_batch)\n",
    "        self.backward(y_batch, use_gradient_softmax_with_loss)\n",
    "        for layer in self._layers[::-1]:\n",
    "            layer.update_weights()\n",
    "\n",
    "    def fit(self, X, Y, batch_size, epochs, shuffle=True, X_val=None, Y_val=None, use_gradient_softmax_with_loss=False):\n",
    "        size = X.shape[0]\n",
    "        X_train, y_train = X[:], Y[:]\n",
    "\n",
    "        self.loss_train_history = []\n",
    "        self.loss_val_history = []\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            if shuffle:\n",
    "                p = np.random.permutation(size)\n",
    "                X_train, y_train = X[p], Y[p]\n",
    "            for step in range(size // batch_size):\n",
    "                ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "                self.update_weights(X_train[ind_slice], y_train[ind_slice], use_gradient_softmax_with_loss)\n",
    "            train_loss, train_acc = self.evaluate(X_train, y_train, batch_size)\n",
    "\n",
    "            if (X_val is not None) and (Y_val is not None):\n",
    "                val_loss, val_acc = self.evaluate(X_val, Y_val, batch_size)\n",
    "                self.loss_val_history.append(val_loss)\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                print(\"Epoch: {:d}, train loss: {:f}, train acc: {:f}, val loss: {:f}, val acc: {:f}\".format(epoch, train_loss, train_acc, val_loss, val_acc))\n",
    "            else:\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                print(\"Epoch: {:d}, train loss: {:f}, train acc: {:f}\".format(epoch, train_loss, train_acc))\n",
    "\n",
    "    def evaluate(self, X, Y, batch_size):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"X and Y must have equal size\")\n",
    "\n",
    "        Y_pred = np.empty(Y.shape)\n",
    "        size = X.shape[0]\n",
    "        for step in range(size // batch_size + 1):\n",
    "            ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "            Y_pred[ind_slice] = self.forward(X[ind_slice])\n",
    "        losses = [self._loss(Y[i], Y_pred[i]) for i in range(size)]\n",
    "        accuracy = (np.argmax(Y, axis=1) == np.argmax(Y_pred, axis=1)).astype(np.float64).sum() / size\n",
    "        return sum(losses) / len(losses), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        n = shape[0]\n",
    "        return np.random.randn(*shape) * np.sqrt(2.0/n)\n",
    "\n",
    "\n",
    "class ZerosInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        return np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(abc.ABC):\n",
    "    def __init__(self, input_dim=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = None\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.d_inputs = None\n",
    "        self.d_outputs = None\n",
    "        self._optimizer = None\n",
    "\n",
    "        self._is_build = False\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        self._optimizer = copy.deepcopy(optimizer)\n",
    "        if prev_layer is not None:\n",
    "            self.input_dim = prev_layer.output_dim\n",
    "        elif self.input_dim is None:\n",
    "            raise ValueError('Input dimension is not determine.'\n",
    "                             'If this first layer, please, use param \"input_dim\"')\n",
    "        self._is_build = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if not self._is_build:\n",
    "            raise ValueError(\"Layer is not build\")\n",
    "        if inputs.shape[1:] != (self.input_dim,):\n",
    "            raise ValueError(\"Input shape is not correct\")\n",
    "        return self._forward(inputs)\n",
    "\n",
    "    def backward(self, grad_outputs):\n",
    "        if self.inputs is None:\n",
    "            raise ValueError(\"Forward pass is not performed\")\n",
    "        return self._backward(grad_outputs)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: np.array((n, d)), input values, n - batch size, d - number input features\n",
    "        ---\n",
    "        output: np.array((n, c)), output values, n - batch size, c - number output features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _backward(self, grad_outputs):\n",
    "        \"\"\"\n",
    "        grad_outputs: np.array((n, c)), gradient by outputs,\n",
    "                      n - batch size, c - number output features of this layer\n",
    "        ---\n",
    "        output: np.array((n, d)), gradient by inputs,\n",
    "                n - batch size,  c - number input features of this layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units, input_dim=None,\n",
    "                 weights_initializer=None, bias_initializer=None):\n",
    "        super().__init__(input_dim)\n",
    "        self.output_dim = units\n",
    "\n",
    "        self.weights = None\n",
    "        self.mean_d_weights = None # mean value gradient weights by batch\n",
    "\n",
    "        self._weights_initializer = weights_initializer if weights_initializer else HeInitializer()\n",
    "        self._bias_initializer = bias_initializer if bias_initializer else ZerosInitializer()\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        weights = self._weights_initializer((self.input_dim, self.output_dim))\n",
    "        bias = self._bias_initializer((1, self.output_dim))\n",
    "        self.weights = np.vstack((weights, bias))\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        inputs_bias = np.zeros((inputs.shape[0], inputs.shape[1] + 1))\n",
    "        inputs_bias[:, :-1] = inputs\n",
    "        inputs_bias[:, -1] = 1\n",
    "        output = np.matmul(inputs_bias, self.weights)\n",
    "        self.outputs = output\n",
    "        self.inputs = inputs\n",
    "        return output\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        output_bias = np.matmul(grad_outputs, np.transpose(self.weights))\n",
    "        inputs_bias = np.zeros((self.inputs.shape[0], self.inputs.shape[1] + 1))\n",
    "        inputs_bias[:, :-1] = self.inputs\n",
    "        inputs_bias[:, -1] = 1\n",
    "        self.mean_d_weights = np.matmul(np.transpose(inputs_bias), grad_outputs) / (grad_outputs.shape[0])\n",
    "        return output_bias[:, :-1]\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.weights = self._optimizer.update_weights(self.weights, self.mean_d_weights)\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        output = np.maximum(inputs, 0)\n",
    "        self.outputs = output\n",
    "        self.inputs = inputs\n",
    "        return output\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        output = grad_outputs * (self.inputs > 0).astype(int)\n",
    "        return output\n",
    "\n",
    "    def update_weights(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        inputs_exp = np.exp(inputs)\n",
    "        output = inputs_exp\n",
    "        for i in range(output.shape[0]):\n",
    "            output[i] /= inputs_exp[i].sum()\n",
    "        self.outputs = output\n",
    "        self.inputs = inputs\n",
    "        return output\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        output = np.zeros(grad_outputs.shape)\n",
    "        for i in range(grad_outputs.shape[0]):\n",
    "            tmp = self.outputs[i].reshape(-1, 1)       \n",
    "            matrix = np.diag(self.outputs[i]) - np.matmul(tmp, self.outputs[i].reshape(1, -1))\n",
    "            output[i] = np.matmul(matrix, grad_outputs[i])\n",
    "        return output\n",
    "\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедитесь в правильности реализации вами функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "INPUT_DIM, OUTPUT_DIM = 4, 6\n",
    "BATCH_SIZE = 3\n",
    "COUNT_TESTS = 10\n",
    "\n",
    "INPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "WEIGHTS = np.random.normal(size=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM))\n",
    "LINEAR_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM))\n",
    "RELU_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "SOFTMAX_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "\n",
    "LINEAR_OUTPUTS = decode_answer(\n",
    "    b'wGm3fs21079DBbbsam/fv0YsX4hX9/o/Nxh4K63n8T++UomsDXPrvxbi2hKA8gjAl03DtDo1AcB05j4D6Bvcv1d2xALzbwBADAx/'\n",
    "    b'H0G0AkD31QxMgaX0P+gv0xPBnfK//f64pm5VBUDLFwHUBV4EwAiYgNzWLA3Ax4deeVK9AsDYJpe26kwEQGVn2t7ZWwLA/HC9xMss'\n",
    "    b'9L+IeOKWVevZv5Rfd295DbI/WHvyPVAW979E6GKRpFPmP8x4zXNcUwTAqCRqOrLM8T/151+zeYr+vx6bEJiWGNC/mhc6p5SqAMCh'\n",
    "    b'SuMGWLDcv24FuPkfPeU//mbyah5d9r+ucwIVIU3oP81dEoOegQVAyKhCIITQDcAwTKAcRY4NQPjHLQNXwALAestiiX2EBkDemn7g'\n",
    "    b'j8DwvzBqu0YGnPw/wNLLm4gz8r8MoZpd8svwv/70sbu0xP2/4Z0Nh94+AMAow1b45oIKwMdHYH/MvwrA506PZHXj4z9aLm5Z2+3s'\n",
    "    b'P8Tk/paM3gZAqCclSm3M+z9pzl5fkHIYQBkKiIdTjRJAkWkBGd358D/IbqQCKN77v7JS+XYenvC/Bo9V1xLnEEAkelfNQC3kPwGY'\n",
    "    b'/nu93/8/sE1Qipm89L/QNoNnBFqkv0TCcC86m+I/tLP+xucZ0j8QA6VJV2HiPyc1Ru5nFgJA5efyWSarA0Cc5Fqc2qrNv2vZQdXk'\n",
    "    b'sQDALOJVE8sv/j/mrk0EYifjP+4ZCcQaJPc/V0d//SUe5b+gPNHw9VzSP/hOIius5gTANiVWAomHFcD7wcLEf3Dwv8EPRV+UIBlA'\n",
    "    b'KHyQtLH1+T9gr+67iXUGQHyz3s0wTNE/1vmK3JcAD8AxuOL3pTL4vwVlfP0a6xdAKk4O0RKRE8DzJk+ETJP1Pwml9+piFgrAsEbz'\n",
    "    b'ANbAC8AYXacpBbXOvxRlJ9bEeOo/yaILuC6LCsDiNBbTRO7Vv6RKEPT8Avu/2uPPVplQ0j+jPgFnam6xvyGX5u0PhPC/X2dnCYQR'\n",
    "    b'EsCgGkfU3kHgvxQ0Am3FNss/V6Y63wHpC8A7tmYislPcPxQ+0M8ZYhFAQIPzNBpF+r9Yv2NY7SoQwOyBsYbZNvI/qLCJjvaL/D9S'\n",
    "    b'O4Mv3CS1vyftMo9OnAvAhP5pHLgYFcAg3GyiFC/pP+R7Q28JJ9C/p96cZCbV8z8pY0LfW6vpv2tGIwTKyxFAmtN1h/cK5T/uR0Ff'\n",
    "    b'LpEEwIOjB57uAfw/iH3ZhwYzB0BipUya/4/pPyz9FjDi5hdARbRzUdzv7j+YYMxaOSILQHANapLprda/IlSg8sQ28D8xDBK2gxnm'\n",
    "    b'P5CW0s+gnbY/gDZeg85Czj9SuLWI8vwLwHZfxhQpCN2/ir6FkxvEEsCShlKitBX2v9rq22/+hvO/QiehO1HX0j/eww3Ft83nP6Vs'\n",
    "    b'dgmGtAZAlP8lO1X99D9QydHZ4nXvPwZiWA6Ssea/TJJIWptA9r/uWjqs06v+v9An3PViyO6/KrQViu7RCcA+Kh7hTMu/v2zoa2ZW'\n",
    "    b'jBXAHrrgqgSM/j8gvM4nNqT3v4xtpLcZYRdAfoxz/N45DsA80sT+DY3yP15FUTkjIPo/oLiF9fzN9T94wKgxQHy9P4DTllkw19O/'\n",
    "    b'MtKPpb+g0b9yPM4rr97iv6xle4TvzP2/0gweWXoCDEBU3w4wKx30P9hmJE5kSAbAJkOcNM34G0BNIxfvXRcTwKb78y1erPe/lJ/Y'\n",
    "    b'redW+L9e47yfGZn1P0ZEwUoRiQHAzIFyAjDGAUC8bw0idVf2vyJwOBRMmAFAL+NuZOhDC8D+5VZNXyoJQLCicm3ysPW/SB7Coj/M'\n",
    "    b'2T8sjM1DuEzrv+rph7ycMhFAPyaGKEi3E8A02vtrHaXqP6L4cDNZe+y/c/Ys2fxu5r8nf4qpwmcSQIi0Q9ysMPI/Fh10yO1sDUAG'\n",
    "    b'6RLr7PUDwG5OmwH+59o/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM)\n",
    ")\n",
    "LINEAR_D_INPUTS = decode_answer(\n",
    "    b'P2edUgWR+r/bzKvlk3bjP9szpDHiifi/LYE0nMt2779h0J1t7RAMwPm13hRgIvo//0PlFKvn078mY9lXhU7dvzHH2LklFRVAj3BH'\n",
    "    b'TpX7AUDEabx1uEIHwBJTpAf2u9k/2UASQZeD0b8ZiUrHtnD5PzUUC+5tqgHAhKA4PIjR8L98KhExIHv1vyfDZ2OKQPy/z8zIh0tu'\n",
    "    b'DsBRGTYBOEEJwLQRGDDCDRHAq91D5VKlIMD95gLQO6QaQI8ES0rhliJAfZgckVxv9T9G/aTxuLrFvwv4niwXcfg//CNiqtSf67+s'\n",
    "    b'xePaogPqP/8f44x7EwzATjSTlQa21z+fmux6IjwPQGMvrv8zShHAbJQRvmQGAcBPLH4HSQ72vxmsITDMDiNAlDrNEjW99D9csOJ6'\n",
    "    b't7T+P93HoYdoKfM/7k3sVZ6J9r/6iK3y2S7ZP/l1IQvc7QTAFTesB+Nb9L9kIG01nPj/v++DG90/aeA/bOFczA0HAMBKeBXJECcV'\n",
    "    b'QEJNe+3vpBvAnwLL8efe4D+bRDTq7ezfv21YsH6B+vM/L6CZoBR21D8hMDDp4yT8v6XULQufIPY/s7GQkbha+b8exktRol0VQAQM'\n",
    "    b'qBQ/ZeI/qeq5APRYjb+lhC8HfqQCwBQiLbLre/u/0xv/24Gi3r801ygQjJv1P2KwMlh6H/u/NUi9uPw/8z8t3X2eBLfzPxLxpw5f'\n",
    "    b'N+c/RyP/A/ah4j+EobO9K38RwPXhyZcCAQFAVmb/UNjVB8Cd/+qN1o0OwK8yktnHcAjALgt4XYrD+L93CbzwJHv7v/0mhN5VO+u/'\n",
    "    b'SzTIYQ0H4j8Mc01xk1QPQFeOI+NWUQXAsP1mWBWl4r9uhE6QjNMEwGcPimeXce6/yI1g6owq8j9edpTiY4H9v1VBS0rUMwHA3wpn'\n",
    "    b'JKiM+b+gmZUgyMLWP4WXYJ9+lw9A5fhvmrbi5T8tCSJlLwIeQJrtuTutefk/4bkQwFPWEsAgAwRhO+QLwHwagU32TAHA4SMOiYKI'\n",
    "    b'7L+6FngpMMgBQDjYeTf1/Ok/4+2b7LP1+b+gXzZ1qKgMQBElUKRe0tW/cOWiD+H+9T/QarjGuP/8PzmlXoBeww3AIHsffOvw5b9P'\n",
    "    b'4Y9kZCcKwFfPnIHSDwtAwb567vp4wz9AImNVabrQv1ilXYAKTua/yRTFuDpc0b8Rxpcdo6QLQFhZ/XLpVQfAzrFbKSnx8b/4OfW8'\n",
    "    b'ovQFwL295MDX1Oo/pTQXX+ZsAMAKax04xwvwv8jcCidyEgLAYlrJn2I4CUBWSMjMKagOwAkewz9Jnum/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "LINEAR_MEAD_D_WEIGHTS = decode_answer(\n",
    "    b'arfqHIAf479QsLlR0m7Wv+jN7ldX6+y/BpChF3zO4r+iO2bPKy/kv/2bdU6oisS/DiFD39Ma1L99QrcdZLrNv0KzC2HEb/K/n35H'\n",
    "    b'WJ2J3b83YGFa7N7kPyNAC1DDoqS/+CC4vvv+zj9Das+5yKinv5tyGRSeGMI/OWWympzx07+Ftb6M3CfgPxMqUKMHYOk/Nb1MLbVm'\n",
    "    b'6D+MCqeZckvXP0VSDrTOPvM/QNo7F1dP4D9TTSLALTPmPxnZ6L6n8uE/3Eoyzo780r+fv7SODo61v9CWDJI137k/4DEvqKkblb+7'\n",
    "    b'URIg42vxv2kD6okB3NS/1AllJjKQ6z98C8dIGr95P81bRcd3OMa/BFQe+lkr4L+/6OnoQdOjPzF9zjIkkK4/FIdNlRa5yb8VCkvn'\n",
    "    b'xhHBv9Uf8azEtL8/ynoNTAMDoj9/VXgneSTDv6jl+qMRrsq/d2DyQa0S+j8ZYBX9Kd3Wv3PPiSI6+bC/hABDe4iF67+1RrhSNQnX'\n",
    "    b'v4wdMNwj5tW/7ADbBBAx47+VPRR0msPVPzEC2dIDlrO/S3U26I2L6T9w8uvHkfbTP9VwiTbq7+A/J46sCQnu8L8r2SLRrSZ8v26W'\n",
    "    b'ATJ6fMM/i2TxxobCsL8doERHZ/aiP8NirrEgmtC/Y/YOGLrk5b+tous4Fn/Tv4hZTNPnst6/QTwB4nVetb+/ehcpxL3HP/EEdNKq'\n",
    "    b'lKs/aCawyDkg2T9VvYpYnG6nvw9Oou01DM0/B26MkVIuwj9VcuoOy1/wv+aK6ClOqdS/GZdYPeuC7L9PJw3O2YPnv0yt1binSPW/'\n",
    "    b'AvgQvByj47/p2QORQK/bP5kxgHJOquq/Vdq8oh/F6j+j8vUqhWvaPzIRwuMfwOA/DCCeqHZohj99+R1SI2S/PwoU1KtTfLC/YGIZ'\n",
    "    b'rKge77/DRA+HjlO3v9P2TkRFW9O/GBq7h6bRqD9b951ij0jtP0vtuEvb9uc/EXFrNq2E0D+D5QuRJ7LaP4OWI2iSLtK/8VhmW9yj'\n",
    "    b'6z+dFaRn5u7YP/PEW9U1tdy/bPa3t4Bz078x2BNPJKPRv4tp66+QyMc/SVuo3FgN1L8J3jF1nFGaP3TnWOkLt+c/2p2TlN2b5L8o'\n",
    "    b'wi8r7ofUvxs0iRHaQuQ/91Qlr9zt5L8MdWywPQGvv7Qm8jXTfec/Yz3Vr3y41r9ZP+2CQdbEv8rYDpBmOqA/l1NCINeT0z8JH8QE'\n",
    "    b'xXXgP76jPbRvg/M/oSJ5KdU81D8VQK3fVCyeP2vMKOhM56U/BZzrQA065L+Y7tDchoDmv0octzYo2PK/09Tl2se20L+VR9p6w6ng'\n",
    "    b'Py97+8+YdO8/GY2Ne+aDxT//8cj2z4qnvyWuZEad4fU//Jk/MSbXxL+8SsKLs3LgPxx0RhYjCu8/rOc0IsdnuT9Il9BhEieov3dg'\n",
    "    b'rP6KYPM/32QVYgu+0z+tURTx01/evxOE+J83UOm/SOaIvyUquL98UvdfsyO1P3Djfkny2/a/b+oz3DbMwb/U+UYpwcjVP3wfVR7E'\n",
    "    b'0Hk/odno8Hwq4r9XLF9XhDrSv6mFfNKLPPU/8HTl7HDNqr8P80KavPfePzXrvspu5+k/cQNiV0l3tb+biTHuj2u2v/MKMn9aIPE/'\n",
    "    b'i4Sjw58d5z8JlSifqwagv3cTV0MnJsm/zwNNPCVt8z/4vZrAbNHVP+gMHM9KvNo/KaoVfEMGAcCPuG9bwtziv8QXBlKdu+4/QzGT'\n",
    "    b'GegB+b9lyoyc+1XoP+8L5L0RquO/ud2z2b/s8D8eEsYVvIjiP1vPVU+/Cey/vFGN6Z9Y5r9vdHeb1az1v9H53gnWF72/gAvQjsoI'\n",
    "    b'AMCPnUx6VQnnv+NlhZ7DnPE/NC7dg5i/5r/HnktUju3zP3f4u/xaCde//W+EBL/C8D9h+aILAbbgP0OyioqCqey/WWjYgB0R5r9z'\n",
    "    b'c30eDzz0vwTjxye+sbq/ShLZt0cwsz8ovJGv+SHkP4Vl9csAA7C/rpcmJq4Z5b+EfET8lCvHP26w3mw9f4S/M9Cb/myByr+XP+IA'\n",
    "    b'c4XevwmdPXKelpo/3aoZilJwzj/5E1EgvB7Avwkbzzl1q6u/G4Wj3MoP5T+FnN3skvfmP8XZHeUb/ZM/59vxTkuRwb8jL7TU7aTR'\n",
    "    b'P1XrcMOdsrk/aWPI3zX0tb+3s6jTXT3Yv9mqBKsh2K0/uJXE+4YR6b9MJvsDFPbLP/Gx8J+Vdte/mX7Xh41b3T/D+uSIDyTlv4DA'\n",
    "    b'qy2aRcQ/tdjMwp6p7j/ldmmKBnqhvxVipc8j1Y8/kY6ZUT/82j90Z6cf+vKfv/0jJMafAtg/t8+tW7Tbvj+454FNAAimv1kdWjA2'\n",
    "    b'XOi/CzZ2TdEo279o8RYchjq/P0OZtGmgxtO/JezwgRUfyL83blLkT2yhP/knW//60uo/1fEmQXrO5b/2+0nprcHRv3vLkB5zzt6/'\n",
    "    b'19DiGucd3T/M79EZ2qXmv63cP7s/kdU/MSsrLva51b+MhISLNzjdv7Kw9U8d0eC/geJJfPp72j/zlqT5CIXMv7fb8xOe1IO/bkbV'\n",
    "    b'wBEO0T8lJPnG377gv+o7CHnhleC/1Qhtj/ZJpT+IJwjf7srqPwG3ZFVGus+/H3VAagNd1L/AxlnIbfr2vzYGDaCWLtK/B5QBlpJZ'\n",
    "    b'9b/BhKXLoaXjv/x0heWipOi/Sem05AlN5b/fK/5MeT7ev/XH6nI1a9K/28QbyMIu4r+cuOK2SWXyvwSSD7LmPcm/4cdGYiw62r80'\n",
    "    b'ZFYQs/0DwKmgrBRly9a/4J3ru6g3BcAcQAUQu8b2v2C1VwG7Pue/7swLWfwq0z+rJCsbpIPNv0MB99PE9sY/P2TEcWzp4b/oRKlM'\n",
    "    b'/JXKv5+O0sNMe+I/qz3oRYwh6L94gghIDufvv8xcrZb619i/nABHslxf8L/bB/6O4I30v+t/YsOD0uC/bF58i6Iv3L8B+DE7Wsb3'\n",
    "    b'P1/XDYzjKOO/CvVrQwC/4b+zkB8mNBndP6N8te73c4w/LaByY+YuvD++sT3s1H7Qvzz04N5Lesu/YnFoVUVdkr85E4jMjraXvxGb'\n",
    "    b'kuucnJq/dUkB69gLtj9fm91p0qDVvzQrCT8+oNM/V7k2cw/vuz/PbjpBIAi3v8mUtCcw/dK/O6P/YvXzxD9yZgOdPh3iv34KQUqg'\n",
    "    b'adM/atEm4hTJxD9EZC3renrCv4JwkSmHAtO/X2JvdLjZ4j+RROs4lg38v30HXm6IBck/a3HT1JvmxT9tLRZWofnRv6x/rrVosvO/',\n",
    "    shape=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM)\n",
    ")\n",
    "RELU_OUTPUTS = decode_answer(\n",
    "    b'YiyQmO8F3j8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACg2CiUz2TzPwAAAAAAAAAA5CtrTHaEvj8AAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAADJCr78bJvE/WS8D2voW5z8AAAAAAAAAAAAAAAAAAAAAaQxgASdm0T8AAAAAAAAAADv4UNwHJeI/V/k5EMmt'\n",
    "    b'0T8AAAAAAAAAAAAAAAAAAAAAIy7b6g8YvT8AAAAAAAAAAKTzoe+yzOA/ISOwt7Dm2T81IbokKXfiPwAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAACSemmQTAnrPy1VcWBaNvE/AAAAAAAAAACF50HKCEz6PwAAAAAAAAAAwd68x2zZ1j8AAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6952Oveto/vCtLVNO00T8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAXxHvVLep7D85r0oej8TpPwAAAAAAAAAAt63DYXGGBEDB+lOtbOb2P/bXRrHncfU/AAAAAAAAAAAAAAAAAAAAAJBN'\n",
    "    b'ZdUcS9o/pBgdwQ8L6j8av8TZeuXAPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHkBh1wAFfI/AAAAAAAAAAAAAAAAAAAAAAaRztEK'\n",
    "    b'uvk/2udSvApj8D/gzG3RNDriPwujSd1sB+w/AAAAAAAAAACoQyp30y7vPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgTDylweOE/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAzBuD87oug/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEltIyyUPmP2Oqm8b53tU/iBCS'\n",
    "    b'chK27j8AAAAAAAAAAAAAAAAAAAAAtdJt8fMqwz8AAAAAAAAAAIi+ZPDzAeY/DW95bLqVxj/pR+S/0s/ZPwAAAAAAAAAAPraYpdBN'\n",
    "    b'0z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVSfL4M2f3PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHGHHIgYB/U/9MSA1TkZ5j+m'\n",
    "    b'K3z4Rd3vP33JHTebLANAKp/kCql0jj8w2YC2AtwKQAAAAAAAAAAAAAAAAAAAAACmls8Rbq3sPwAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAACSn9qtmWNg/oXMhsVm4tT9KDGAkRqzbPwY2K17MUfg/AAAAAAAAAABnNImjpzTjP5ENrHP7jNE/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "RELU_D_INPUTS = decode_answer(\n",
    "    b'A2itoRO18L8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAHh/vZZPO/PwAAAAAAAACAWRIg0o5y1b8AAAAAAAAAgAAAAAAAAACAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAABE1RhH4o8o/KgQl1O5e5L8AAAAAAAAAAAAAAAAAAACAuuBfgdfy3T8AAAAAAAAAAOFo1PooyfQ/kqsXtUZd'\n",
    "    b'7z8AAAAAAAAAgAAAAAAAAACA0Ev2eU0CzD8AAAAAAAAAgDOyhn98d+k/RvVD7zz+sj8PYlvxi/bPvwAAAAAAAAAAAAAAAAAAAIAA'\n",
    "    b'AAAAAAAAAAAAAAAAAACAAAAAAAAAAAAS5p+ceXSxP4Kfa3xtPuk/AAAAAAAAAICRu8tJJc/tPwAAAAAAAACAVNcnv2uq8r8AAAAA'\n",
    "    b'AAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFqConRSR+M/qWsKZIi7+b8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAARSt0hcOH1L+esNsN7Yn+PwAAAAAAAACAez+0wK/A3j8Itz0J1tjvv/ivP2za98a/AAAAAAAAAIAAAAAAAAAAAMVB'\n",
    "    b'rpc6rOk/hzNsPsw04D/jrFsZmwX6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFLdAtr9fvi/AAAAAAAAAIAAAAAAAAAAAJkqln2f'\n",
    "    b'2t0/IChXD6IWwL+9Ou0Wx5z1Pz3sQ2fQrvO/AAAAAAAAAIDiIBDEYqMCQAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAANEfbPk1BKY/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAEvJuYm5Gt4/AAAAAAAAAIAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAB9zMZQP/PJP8BXEKpVDag/O7Yp'\n",
    "    b'MAiSwD8AAAAAAAAAAAAAAAAAAACApRbFTUOw6b8AAAAAAAAAAAG3xBTkt+y/vEFvt/tC/T+ejuHmq+/jvwAAAAAAAAAAKMW0BHXh'\n",
    "    b'1T8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAICXmOxlXy7nPwAAAAAAAACAAAAAAAAAAAAAAAAAAAAAgPAWWwNPlIS/i9lVufSUz78O'\n",
    "    b'xmPOGt72v8/IkgrUPvS/3AyaWWXA5j9JW3lZsi7qvwAAAAAAAAAAAAAAAAAAAADWcSkNqkPUvwAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAAC6UQFNDQOy/SSAyS6fH/b8/A8h1B8Lnv1G0bA9o/v6/AAAAAAAAAIBUspOra+zrP+v3rx0Rmvi/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_OUTPUTS = decode_answer(\n",
    "    b'xXhAbjms4T+kt8Giv6nQPzVFkyFmjrM/DBZi4c9ovD+9/2O0t+7iP44M7/No88I/U3O850ljyT+nBBJLubmvPxT87vyvibo/okfs'\n",
    "    b'spSinj/Z0qx5mibDP4qJX7wO8OY/l5LFH5o63z9Thj+aifG9P8BtGLOBd7U/ZXDkDCPr0z/IwiZOFI3EP5s57yGFtds/TWOx06O3'\n",
    "    b'1D/UBjCNMzG1PygFI9W5Y8I/d2S+q7Q01D+XoIyKpHKwP83wDEfFfN4/TZGKROcn2T/Z7S4nI+PdPxWaCFi/Kag/ZbYVpfa+tz/j'\n",
    "    b'y3AyoZfIPwqapYg9YrY/+uXC6quTsj8GvfaEWrvkP7TEueeHU9Q/U4IzYvzcuD9ssTWdee7hPxF+31RchJk/+H6EohVj4j+p1hFy'\n",
    "    b'9jbKP2yoaJx9abE/RtmnNfSHwz+EAnm6CdCyP3NM4EVc9dk/Phc5Ih2i1j9dt5BSCGnFP/e/vb1UidQ/etZ55g/8zD+ImpUDiLLU'\n",
    "    b'P4x035Y2jMA/EQnOG8JrwT82R0KU99O/P+R3sWe3B5E/FqlGy1Ii5z/OMhEaAWPdP0XDudD/1No/itb+kyNvoT9zvNSKami2PwTx'\n",
    "    b'EuhvENI/Gp3wsa8H2z/usO6aUlbLP6RlFGLc8rQ/puBG0pL5yT+xPT3zjBmxP2jJA2Dbs+U/oWst0OSoqj+hqzDFonKZP4UIrPFZ'\n",
    "    b'UuA/WIhcsW810j/Gt/B9ZB3HP5yWGOW8Bd0/WroVVtIvlT/YhpTupwPgP7UA0IVh/5k/orWs6WSIxz+sf3qzlWPQP3KcH9Yqm94/'\n",
    "    b'TiQ+BjT0tD/0gvjjYly3PyWmjDoWgeU/RA5ViJ4ftj/SnqZfpj3DP16af68gyMU/6Wf0OUhMxj/U69O12KfYPwkTctXyTdE/IWc2'\n",
    "    b'XM0B4j9ShSGuPi2yP3R+u6ZNrr0/z/BbMoIF0D+4D+jmodq3Pwm+Bal1rNg/IvxNqzOYzT/5P5kHyJDSP8mZuU3jR9U/d73rJi3M'\n",
    "    b'4D8+PRtUOHmmP4IOrmdtQrk/wpkNSXWMsj/UJG9Abf7pP8SFlAqk2qA/vfwurs4StT8zl5ubCdmzPyHFqQjqFdo/duHz1n72yz9X'\n",
    "    b'ZPUklPjSP8wdLsKQ99A/YYHty7gTmT/vJlfHtCvmP/PDSyai/JE/67D3fQJksT8qxg5hUlbiP3Ws7SgtYsI/amLbE4iSyz9NPV2T'\n",
    "    b'XXKbP4+nZp+yQ9Y/s4VBEkSR0D/p/iF143PXP4B2/H6DleE/eUUAMLjHsj+sRXJKPgnMP5q9m6HXPMQ/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_D_INPUTS = decode_answer(\n",
    "    b'WIS5wPxs4T/tYtAttQTgv9cCAzg36bG/Et7nguqbmj+RCJAVjLXIPzxW5KylgX2/ipaHs5Vswr9MPYXSpHOlvw2PEG2zW7y/JwA4'\n",
    "    b'6NU4ab+dTU/lBrmwP+ACBm7m2Kg/B/NST+5g3r9C2RHZsTzIP/yg5DkmurI/T7yhqBcoyz9vVDiNFadzPwpePGQmasi/iCAwIL87'\n",
    "    b'wT+C64leukSqPyixnj8Xe8I/k4hfLUpN0j9E/V2gBuGyPz0wo7qLIeC/r/IQwL1l3D/X0+Wf2/zhvxO113eJf7E/0j0mDrmgqT8m'\n",
    "    b'x0KV81DSP+DxONaMWbA/16nv9zr3tT8U7syIJeXbv6+7H1CZlbW/+coU9Og+vz+ZgoyXbui0v5nmLuc9fqY/dXkJ2kNvxL9McPuv'\n",
    "    b'eP7UP9BQ3+dJ66s/WTvl/3+IzL/IA9At1d6yv0xMvOWEcMo/2sbh6jB90b9LQ++GR/nBP/9ZUvw3m9s/GAO8VhNR3r/POg6w6VjA'\n",
    "    b'Pz3RdfZl2rW/qtEOh/cbuj+msDlAwpKyv2ydqxZGD6K/ANsKSNzmdz+QFp++o3vTPzcOtL/SJ9a/OQROdZy5ir9EPvsl3w+sP/ib'\n",
    "    b'Zs5/AsI/B6RNRqRM1D8Q8otoA47cv17+n54Y/Ie/rhdKMI9awT9+ILwcdYOuv8SymrvdwLi/aE6Spec1lT/4QPSzqMCevxRLlmqf'\n",
    "    b'Hsw/NqirnEao0b/0Gn/KBRS2Pz0YL5M80dO/jlzx7lGWgr/hgPyMrDDTP9wjolopVJM/NsC9m/sZur+zRdRP4eW3PypKj8jrO4+/'\n",
    "    b'Go/tE19umD9VAUaMLreoP73ULAeK0bw/jxu5zPBKsz99eAQNCTzOv2omI7+oHtW/OOU1Jclfx79YIGD5GNykP/oUkjIKM94/dtv2'\n",
    "    b'Bq3v2z+NWtKJvHm0v355k/lF6sW/MBBxzzW4x78PMyqAB/iYP/TdmjfU6am/gKn04JArmT+A9W3hAAFbPzf5e97n9bG/aDwuGZW5'\n",
    "    b'vL/eZWZDPuORv4vnQUQmlMk/sNL224W1mD9wTPCp6MeJPwyOtgLSt7c/IiZ5RxiPwL+Rrgnnpyinv3MTNHvFG8M/iGt/imN+nD8/'\n",
    "    b'lcHyZ+HAv31Hy8mFneG/gyw+x8uBkL80oAPJPw7jP1jqzB90lZ2/04p+0+MRnD/qQP8J1fvFv20rlcJDLJy//hTiByH/xT/qnEBS'\n",
    "    b'KCJ/P/B+GpViZ5C/eIHg7ViN07/0Jul1RhfUP8KBvfGu5bS/tIGJ1Kx4eD/V5GOKk927P2T23cve/qC/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}:\\ntrue values:{},\\ndesired_values:{}\"\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    np.testing.assert_almost_equal(actual_values, desired_values, err_msg=msg, verbose=False)\n",
    "\n",
    "def test_forward(layer, true_outputs, set_weights=False, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, true_output) in enumerate(zip(INPUTS, true_outputs)):\n",
    "        if set_weights:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        desired_output = layer.forward(input)\n",
    "        msg = ERROR_MSG.format(i, true_output, desired_output)\n",
    "        check_answers(true_output, desired_output, msg)\n",
    "        \n",
    "def test_backward(layer, true_d_inputs, d_outputs, true_mean_d_weights=None, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, d_output, true_d_input) in enumerate(zip(INPUTS, d_outputs, true_d_inputs)):\n",
    "        if true_mean_d_weights is not None:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        layer.forward(input)\n",
    "        desired_d_input = layer.backward(d_output)\n",
    "        check_answers(true_d_input, desired_d_input, \n",
    "                      msg=ERROR_MSG.format(i, true_d_input, desired_d_input))\n",
    "        if true_mean_d_weights is not None:\n",
    "            check_answers(true_mean_d_weights[i], layer.mean_d_weights, \n",
    "                          msg=ERROR_MSG.format(i, true_mean_d_weights[i], layer.mean_d_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_OUTPUTS, set_weights=True)\n",
    "test_backward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_D_INPUTS, LINEAR_D_OUTPUTS, LINEAR_MEAD_D_WEIGHTS)\n",
    "\n",
    "test_forward(ReLU(INPUT_DIM), RELU_OUTPUTS, set_weights=True)\n",
    "test_backward(ReLU(INPUT_DIM), RELU_D_INPUTS, RELU_D_OUTPUTS)\n",
    "\n",
    "test_forward(Softmax(INPUT_DIM), SOFTMAX_OUTPUTS, set_weights=True)\n",
    "test_backward(Softmax(INPUT_DIM), SOFTMAX_D_INPUTS, SOFTMAX_D_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Обучение нейросети стохастическим градиентным спуском"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теоретическая часть \n",
    "#### Градиентный спуск\n",
    "Настройка весов нейросети может быть сведена к поиску вектора $w$, доставляющего минимум функционалу ошибки:\n",
    "\n",
    "$$E(w) = E( \\boldsymbol{y},  \\boldsymbol{\\widehat{y}}) \\to min $$\n",
    "\n",
    "Минимизировать E(w) будем с помощью градиентного спуска. Правило изменения вектора весов на каждой итерации: \n",
    "\n",
    "$$\n",
    "w^{(k)} = w^{(k - 1)} - \\beta \\nabla_w E(w^{(k - 1)})\n",
    "$$\n",
    "Длину шага $\\beta > 0$ в рамках данного задания предлагается брать равной некоторой малой константе.\n",
    "\n",
    "В случае полного градиентного спуска $\\nabla_w E(w)$ считается все объекты выборки). В случае градиентного спуска  с минибатчем \n",
    "$$\\nabla_w E(w) \\approx \\frac{1}{n}\\sum_{j=1}^{n}{\\nabla_w q_{i_{k_j}} (w)}$$\n",
    "где $q_{i_{k_j}}$ — случайно выбранные номера слагаемых, а n меньше общего колличества примеров для обучения.\n",
    "\n",
    "#### Момент импульса(momentum)\n",
    "Может оказаться, что направление антиградиента сильно меняется от шага к шагу. Чтобы добиться болле эффективной сходимости, можно усреднять векторы антиградиента с нескольких предыдущих шагов — в этом случае шум уменьшится, и такой средний вектор будет указывать в сторону общего направления движения. Введем вектор инерции:\n",
    "    $$h_0 = 0;$$\n",
    "    $$h_k = \\alpha h_{k - 1} + \\beta \\nabla_w E(w^{(k - 1)}) $$\n",
    "Тогда шаг градиентного спуска будет:\n",
    "    $$w^{(k)} = w^{(k - 1)}  - h_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая часть\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.** Реализация стохастического градиентного спуска с моментом\n",
    "\n",
    "\n",
    "Вам необходимо реализовать алгоритм обновления весов с моментом. Для этого необходимо реализовать метод `update_weights` класса `SGD`. Также вам необходимо реализовать метод `update_weights` во всех слоях нейросети(у которых есть веса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, lr, momentum=0):\n",
    "        self._lr = lr\n",
    "        self._momentum = momentum\n",
    "        self._h = 0.\n",
    "\n",
    "    def update_weights(self, weights, gradient):\n",
    "        \"\"\"\n",
    "        weights: np.array((n, m)), current weigths of algorithm\n",
    "        gradient: np.array((n, m)), average gradient by weights\n",
    "        ---\n",
    "        output: np.array((n, m)), new weights values\n",
    "        \"\"\"\n",
    "        \n",
    "        self._h *= self._momentum\n",
    "        self._h += self._lr * gradient\n",
    "        weights -= self._h\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.** Обучение нейросети для задачи классификации цифр mnist\n",
    "\n",
    "В этой части вам необходимо обучить вашу нейросеть для задачи классификации рукописных цифр mnist. Вам потребуются методы `fit` и `evaluate` класса `Model`. Можете использовать предложенную архитектуру или выбрать любую другую. Шаг обучения, количество эпох и размер батча выбирите на ваше усмотрение. Посчитайте ошибку и точность(accuracy) предсказания на тестовом датасете.\n",
    "\n",
    "Обучите нейросеть без момента и c моментом(выбранным на ваше усмотрение) и постройте графики ошибки во время обучения в зависимости от числа итераций. Постройте те же графики для валидационной части. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(images, labels):\n",
    "    binarizer = LabelBinarizer()\n",
    "    X = images.reshape((images.shape[0], 28 * 28))\n",
    "    y = binarizer.fit_transform(labels)\n",
    "    return X / 255.0, y \n",
    "\n",
    "def create_mnist_model(loss, optimizer):\n",
    "    model = Model(loss=loss, optimizer=optimizer)\n",
    "    model.add(Linear(128, input_dim=28*28))\n",
    "    model.add(ReLU())\n",
    "    model.add(Linear(128))\n",
    "    model.add(ReLU())\n",
    "    model.add(Linear(10))\n",
    "    model.add(Softmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_data(mnist.train_images(), mnist.train_labels())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "X_test, y_test = prepare_data(mnist.test_images(), mnist.test_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.046442, train acc: 0.874950, val loss: 0.046704, val acc: 0.870859\n",
      "Epoch: 2, train loss: 0.034522, train acc: 0.902736, val loss: 0.035172, val acc: 0.901364\n",
      "Epoch: 3, train loss: 0.030071, train acc: 0.914179, val loss: 0.031333, val acc: 0.910404\n",
      "Epoch: 4, train loss: 0.026942, train acc: 0.922960, val loss: 0.028360, val acc: 0.919141\n",
      "Epoch: 5, train loss: 0.024762, train acc: 0.929925, val loss: 0.026539, val acc: 0.924343\n",
      "Epoch: 6, train loss: 0.023099, train acc: 0.934104, val loss: 0.025046, val acc: 0.928889\n",
      "Epoch: 7, train loss: 0.021685, train acc: 0.938308, val loss: 0.023840, val acc: 0.932121\n",
      "Epoch: 8, train loss: 0.020820, train acc: 0.939776, val loss: 0.023117, val acc: 0.933485\n",
      "Epoch: 9, train loss: 0.019367, train acc: 0.944801, val loss: 0.021877, val acc: 0.936212\n",
      "Epoch: 10, train loss: 0.018470, train acc: 0.947015, val loss: 0.021036, val acc: 0.938586\n",
      "Epoch: 11, train loss: 0.017352, train acc: 0.950348, val loss: 0.020096, val acc: 0.940960\n",
      "Epoch: 12, train loss: 0.016623, train acc: 0.952313, val loss: 0.019454, val acc: 0.942323\n",
      "Epoch: 13, train loss: 0.015947, train acc: 0.954154, val loss: 0.018892, val acc: 0.944141\n",
      "Epoch: 14, train loss: 0.015125, train acc: 0.956891, val loss: 0.018314, val acc: 0.945808\n",
      "Epoch: 15, train loss: 0.014622, train acc: 0.959055, val loss: 0.017934, val acc: 0.946818\n",
      "Epoch: 16, train loss: 0.013929, train acc: 0.960199, val loss: 0.017335, val acc: 0.949040\n",
      "Epoch: 17, train loss: 0.013401, train acc: 0.961791, val loss: 0.016888, val acc: 0.950354\n",
      "Epoch: 18, train loss: 0.012968, train acc: 0.962711, val loss: 0.016474, val acc: 0.949798\n",
      "Epoch: 19, train loss: 0.012432, train acc: 0.965149, val loss: 0.016090, val acc: 0.952273\n",
      "Epoch: 20, train loss: 0.011968, train acc: 0.965871, val loss: 0.015720, val acc: 0.953384\n",
      "Epoch: 21, train loss: 0.011553, train acc: 0.967438, val loss: 0.015439, val acc: 0.952879\n",
      "Epoch: 22, train loss: 0.011174, train acc: 0.968806, val loss: 0.015086, val acc: 0.954545\n",
      "Epoch: 23, train loss: 0.010804, train acc: 0.969527, val loss: 0.014711, val acc: 0.955707\n",
      "Epoch: 24, train loss: 0.010353, train acc: 0.970572, val loss: 0.014359, val acc: 0.956313\n",
      "Epoch: 25, train loss: 0.010211, train acc: 0.971816, val loss: 0.014500, val acc: 0.956515\n",
      "Epoch: 26, train loss: 0.009764, train acc: 0.973109, val loss: 0.014010, val acc: 0.957576\n",
      "Epoch: 27, train loss: 0.009488, train acc: 0.973657, val loss: 0.013865, val acc: 0.958283\n",
      "Epoch: 28, train loss: 0.009185, train acc: 0.975050, val loss: 0.013491, val acc: 0.959040\n",
      "Epoch: 29, train loss: 0.008923, train acc: 0.975323, val loss: 0.013342, val acc: 0.959949\n",
      "Epoch: 30, train loss: 0.008622, train acc: 0.976020, val loss: 0.013149, val acc: 0.960909\n",
      "Epoch: 31, train loss: 0.008347, train acc: 0.977289, val loss: 0.013064, val acc: 0.960000\n",
      "Epoch: 32, train loss: 0.008144, train acc: 0.977313, val loss: 0.012964, val acc: 0.961010\n",
      "Epoch: 33, train loss: 0.007839, train acc: 0.978657, val loss: 0.012691, val acc: 0.961818\n",
      "Epoch: 34, train loss: 0.007620, train acc: 0.978632, val loss: 0.012574, val acc: 0.961970\n",
      "Epoch: 35, train loss: 0.007793, train acc: 0.978060, val loss: 0.012958, val acc: 0.961061\n",
      "Epoch: 36, train loss: 0.007338, train acc: 0.980224, val loss: 0.012352, val acc: 0.962576\n",
      "Epoch: 37, train loss: 0.007015, train acc: 0.981219, val loss: 0.012118, val acc: 0.963283\n",
      "Epoch: 38, train loss: 0.006942, train acc: 0.981269, val loss: 0.012163, val acc: 0.964242\n",
      "Epoch: 39, train loss: 0.006643, train acc: 0.982388, val loss: 0.011902, val acc: 0.964293\n",
      "Epoch: 40, train loss: 0.006427, train acc: 0.982836, val loss: 0.011664, val acc: 0.964798\n",
      "Epoch: 41, train loss: 0.006215, train acc: 0.984055, val loss: 0.011563, val acc: 0.964343\n",
      "Epoch: 42, train loss: 0.006076, train acc: 0.983731, val loss: 0.011478, val acc: 0.965253\n",
      "Epoch: 43, train loss: 0.005980, train acc: 0.984154, val loss: 0.011500, val acc: 0.965152\n",
      "Epoch: 44, train loss: 0.005742, train acc: 0.985100, val loss: 0.011365, val acc: 0.965101\n",
      "Epoch: 45, train loss: 0.005682, train acc: 0.985448, val loss: 0.011362, val acc: 0.964747\n",
      "Epoch: 46, train loss: 0.005468, train acc: 0.985896, val loss: 0.011196, val acc: 0.965303\n",
      "Epoch: 47, train loss: 0.005358, train acc: 0.986642, val loss: 0.011119, val acc: 0.965404\n",
      "Epoch: 48, train loss: 0.005173, train acc: 0.987090, val loss: 0.011030, val acc: 0.965859\n",
      "Epoch: 49, train loss: 0.005118, train acc: 0.987065, val loss: 0.011104, val acc: 0.965909\n",
      "Epoch: 50, train loss: 0.004910, train acc: 0.987811, val loss: 0.010857, val acc: 0.966465\n"
     ]
    }
   ],
   "source": [
    "model = create_mnist_model(loss=CategoricalCrossentropy(), optimizer=SGD(lr=0.003))\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=20, epochs=epochs, X_val=X_val, Y_val=y_val, use_gradient_softmax_with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default SGD: Test loss: 0.009557, test accuracy: 0.970400\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=10)\n",
    "print(\"Default SGD: Test loss: {:f}, test accuracy: {:f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.035047, train acc: 0.899925, val loss: 0.036010, val acc: 0.894747\n",
      "Epoch: 2, train loss: 0.026884, train acc: 0.922637, val loss: 0.028380, val acc: 0.916970\n",
      "Epoch: 3, train loss: 0.023209, train acc: 0.933483, val loss: 0.025105, val acc: 0.927727\n",
      "Epoch: 4, train loss: 0.020450, train acc: 0.942090, val loss: 0.022908, val acc: 0.934091\n",
      "Epoch: 5, train loss: 0.018105, train acc: 0.948557, val loss: 0.020743, val acc: 0.939091\n",
      "Epoch: 6, train loss: 0.016747, train acc: 0.951219, val loss: 0.019783, val acc: 0.941919\n",
      "Epoch: 7, train loss: 0.015383, train acc: 0.955896, val loss: 0.018643, val acc: 0.944646\n",
      "Epoch: 8, train loss: 0.014177, train acc: 0.959900, val loss: 0.017721, val acc: 0.949495\n",
      "Epoch: 9, train loss: 0.012696, train acc: 0.965025, val loss: 0.016328, val acc: 0.952677\n",
      "Epoch: 10, train loss: 0.011973, train acc: 0.966493, val loss: 0.015724, val acc: 0.953586\n",
      "Epoch: 11, train loss: 0.011161, train acc: 0.969478, val loss: 0.015091, val acc: 0.956162\n",
      "Epoch: 12, train loss: 0.010414, train acc: 0.971194, val loss: 0.014629, val acc: 0.957020\n",
      "Epoch: 13, train loss: 0.010003, train acc: 0.972637, val loss: 0.014180, val acc: 0.958485\n",
      "Epoch: 14, train loss: 0.009594, train acc: 0.974030, val loss: 0.014077, val acc: 0.957929\n",
      "Epoch: 15, train loss: 0.008638, train acc: 0.976841, val loss: 0.013232, val acc: 0.960404\n",
      "Epoch: 16, train loss: 0.008161, train acc: 0.978557, val loss: 0.012836, val acc: 0.961970\n",
      "Epoch: 17, train loss: 0.007852, train acc: 0.978109, val loss: 0.012761, val acc: 0.963384\n",
      "Epoch: 18, train loss: 0.007507, train acc: 0.979428, val loss: 0.012482, val acc: 0.962121\n",
      "Epoch: 19, train loss: 0.007118, train acc: 0.980572, val loss: 0.012382, val acc: 0.962828\n",
      "Epoch: 20, train loss: 0.006534, train acc: 0.983234, val loss: 0.011933, val acc: 0.965404\n",
      "Epoch: 21, train loss: 0.006456, train acc: 0.982985, val loss: 0.012179, val acc: 0.964242\n",
      "Epoch: 22, train loss: 0.005967, train acc: 0.984403, val loss: 0.011663, val acc: 0.965505\n",
      "Epoch: 23, train loss: 0.005794, train acc: 0.984826, val loss: 0.011618, val acc: 0.965505\n",
      "Epoch: 24, train loss: 0.005385, train acc: 0.986119, val loss: 0.011337, val acc: 0.967424\n",
      "Epoch: 25, train loss: 0.005199, train acc: 0.986866, val loss: 0.011208, val acc: 0.968131\n",
      "Epoch: 26, train loss: 0.005192, train acc: 0.986866, val loss: 0.011358, val acc: 0.967525\n",
      "Epoch: 27, train loss: 0.004677, train acc: 0.988035, val loss: 0.011002, val acc: 0.968939\n",
      "Epoch: 28, train loss: 0.004453, train acc: 0.989502, val loss: 0.010830, val acc: 0.968788\n",
      "Epoch: 29, train loss: 0.004501, train acc: 0.988358, val loss: 0.010909, val acc: 0.968081\n",
      "Epoch: 30, train loss: 0.004109, train acc: 0.990224, val loss: 0.010676, val acc: 0.968384\n",
      "Epoch: 31, train loss: 0.003908, train acc: 0.990771, val loss: 0.010679, val acc: 0.968939\n",
      "Epoch: 32, train loss: 0.003837, train acc: 0.991144, val loss: 0.010793, val acc: 0.969848\n",
      "Epoch: 33, train loss: 0.003648, train acc: 0.992015, val loss: 0.010600, val acc: 0.969444\n",
      "Epoch: 34, train loss: 0.003384, train acc: 0.992637, val loss: 0.010497, val acc: 0.969293\n",
      "Epoch: 35, train loss: 0.003258, train acc: 0.993259, val loss: 0.010525, val acc: 0.970051\n",
      "Epoch: 36, train loss: 0.003060, train acc: 0.993905, val loss: 0.010268, val acc: 0.970960\n",
      "Epoch: 37, train loss: 0.002960, train acc: 0.993632, val loss: 0.010325, val acc: 0.971061\n",
      "Epoch: 38, train loss: 0.002917, train acc: 0.994328, val loss: 0.010389, val acc: 0.970404\n",
      "Epoch: 39, train loss: 0.002685, train acc: 0.994975, val loss: 0.010226, val acc: 0.971414\n",
      "Epoch: 40, train loss: 0.002602, train acc: 0.994876, val loss: 0.010306, val acc: 0.971414\n",
      "Epoch: 41, train loss: 0.002479, train acc: 0.995398, val loss: 0.010199, val acc: 0.971263\n",
      "Epoch: 42, train loss: 0.002477, train acc: 0.995498, val loss: 0.010461, val acc: 0.970707\n",
      "Epoch: 43, train loss: 0.002577, train acc: 0.994677, val loss: 0.010472, val acc: 0.971212\n",
      "Epoch: 44, train loss: 0.002232, train acc: 0.996070, val loss: 0.010212, val acc: 0.971667\n",
      "Epoch: 45, train loss: 0.002197, train acc: 0.996045, val loss: 0.010128, val acc: 0.970707\n",
      "Epoch: 46, train loss: 0.002063, train acc: 0.996667, val loss: 0.010276, val acc: 0.972323\n",
      "Epoch: 47, train loss: 0.002026, train acc: 0.996965, val loss: 0.010307, val acc: 0.970909\n",
      "Epoch: 48, train loss: 0.001859, train acc: 0.997114, val loss: 0.010232, val acc: 0.971869\n",
      "Epoch: 49, train loss: 0.001805, train acc: 0.997587, val loss: 0.010152, val acc: 0.971818\n",
      "Epoch: 50, train loss: 0.001679, train acc: 0.998035, val loss: 0.010090, val acc: 0.971818\n"
     ]
    }
   ],
   "source": [
    "model_momentum = create_mnist_model(loss=CategoricalCrossentropy(), optimizer=SGD(lr=0.003, momentum = 0.5))\n",
    "\n",
    "model_momentum.fit(X_train, y_train, batch_size=20, epochs=epochs, X_val=X_val, Y_val=y_val, use_gradient_softmax_with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with momentum: Test loss: 0.008999, test accuracy: 0.974100\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_momentum.evaluate(X_test, y_test, batch_size=10)\n",
    "print(\"SGD with momentum: Test loss: {:f}, test accuracy: {:f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VPW98PHPdybLZN8JZIGEnYQlCIKKKAIKLnXFutalWm/r8mj7tGqfPlX0qb2t7a29vfZWreu1LihuWLFuuKEIBNlBhECAsGQh+57M/J4/zgRDyDLALMnM9/16ndfMnPmd+f0Oxu8557eKMQallFKhwRboAiillPIfDfpKKRVCNOgrpVQI0aCvlFIhRIO+UkqFEA36SikVQjToK+UmInYRqReRocdx7EgR0f7Pqt/ToK8GLHeA7thcItLU6fM1x/p7xhinMSbWGLPHF+VVqj8IC3QBlDpexpjYjvciUgzcbIz5sKf0IhJmjGn3R9mU6q/0Tl8FLRH5jYgsEpGXRKQOuFZEThWRr0SkWkQOiMhfRCTcnT5MRIyI5Lg//8P9/bsiUiciK0Qk18O8s0TknyJSKSLbReSHnb47RUS+FpFaESkVkT+490eLyIsicshdvlUikur1fxgV0jToq2B3CfAikAAsAtqBO4FUYAYwH/i3Xo6/Gvg1kAzsAf6fh/kuAnYBGcAVwMMicqb7u/8C/mCMiQdGAovd+28EooEsIAW4FWj2MD+lPKJBXwW75caYt40xLmNMkzFmtTFmpTGm3RizE3gCOLOX4xcbYwqNMW3AC0BBXxm6nwamAfcaY5qNMV8DzwA/cCdpA0aJSIoxps4Ys7LT/lRgpLt9odAYU398p61U9zToq2C3t/MHERkrIu+IyEERqQUexAq0PTnY6X0jENtTwk4ygApjTEOnfbuBTPf7G4E8YJu7Cuc89/5ngQ+BV0Rkn4j8TkS03U15lQZ9Fey6dqN8HNiEdTcdD9wHiJfz3A+kikhMp31DgX0AxphtxpgrgUHAfwCviYjDGNNqjFlojBkHnI5VNXXMvZCU6o0GfRVq4oAaoEFExtF7ff5xMcbsAgqB34pIpIgUYN3dvwAgIj8QkVRjjMtdFgO4RGS2iIwXERtQi1Xd4/R2+VRo06CvQs3/Bq4H6rDu+hf5KJ8rgFFY1UOLgf9jjPnY/d15wFZ3j6I/AlcYY1qxqoVexwr4m7Gqel7yUflUiBJdREUppUKH3ukrpVQI0aCvlFIhRIO+UkqFEA36SikVQvrdwI/U1FSTk5MT6GIopdSAsmbNmgpjTFpf6fpd0M/JyaGwsDDQxVBKqQFFRHZ7kk6rd5RSKoRo0FdKqRCiQV8ppUJIv6vTV0oNLG1tbZSUlNDcrFP/+4PD4SArK4vw8PDjOl6DvlLqhJSUlBAXF0dOTg4i3p6wVHVmjOHQoUOUlJSQm+vRIm5H0eodpdQJaW5uJiUlRQO+H4gIKSkpJ/RUpUFfKXXCNOD7z4n+WwdN0N9X3cSf3t/G7kMNfSdWSqkQFTRBv6axjb8s28Hm/bWBLopSKoAWLlzIH//4xx6/Ly8vZ/r06UyePJnPP//8mH//2Wef5fbbbwfgzTffZMuWLcdd1kAImqCfmRgFwP7qpgCXRCnVn3300UeMHTuWtWvXMnPmzBP6LQ36ARQfFUZMhJ19GvSVCjkPPfQQY8aMYe7cuWzbtg2AoqIi5s+fz5QpU5g5cybffPMN69at4+6772bp0qUUFBTQ1NTET37yE6ZOnUp+fj7333//4d/MycmhoqICgMLCQmbNmnVEnl9++SVLlizhF7/4BQUFBRQVFfntfE9E0HTZFBEyk6L0Tl+pAHrg7c1s8XIVa15GPPd/L7/H79esWcPLL7/M2rVraW9v56STTmLKlCnccsstPPbYY4waNYqVK1dy6623smzZMh588EEKCwt59NFHAeuCkZycjNPpZM6cOWzYsIGJEyf2Wa7TTjuNCy+8kAsuuIAFCxZ47Xx9LWiCPkBGYhT7q3WAiFKh5PPPP+eSSy4hOjoagAsvvJDm5ma+/PJLLr/88sPpWlpauj3+lVde4YknnqC9vZ0DBw6wZcsWj4L+QBV0QX9DSU2gi6FUyOrtjtyXunZjdLlcJCYmsm7dul6P27VrF3/84x9ZvXo1SUlJ3HDDDYf7wIeFheFyuQCCarRx0NTpg9WYW9nQSlOrM9BFUUr5yRlnnMEbb7xBU1MTdXV1vP3220RHR5Obm8urr74KWCNZ169ff9SxtbW1xMTEkJCQQGlpKe++++7h73JyclizZg0Ar732Wrd5x8XFUVdX54Oz8p2gC/oA+2u0Xl+pUHHSSSdxxRVXUFBQwGWXXXa4R84LL7zAU089xaRJk8jPz+ett9466thJkyYxefJk8vPz+eEPf8iMGTMOf3f//fdz5513MnPmTOx2e7d5X3nllfzhD39g8uTJA6YhV4wxgS7DEaZOnWqOdxGVVbsq+f7jK3j+pmnMHNXnAjJKKS/YunUr48aNC3QxQkp3/+YissYYM7WvY4PqTj8j0QHAviq901dKqe4EVdBPj3dgEx2gpZRSPQmqoB9ut5Ee72CfdttUSqluBVXQB6sxV+/0lVKqe0EX9DMSo7T3jlJK9SAog/6B6mZcrv7VK0kppfqDoAv6mYkOWp0uKuq7H3KtlFK+tm7dOpYuXRroYnQr6IJ+hnuAls62qZQKFA36fpSZ1DGvvvbgUSoUFBcXM3bsWG6++WbGjx/PNddcw4cffsiMGTMYNWoUq1atorKykosvvpiJEydyyimnsGHDBsBacOX666/nnHPOIScnh9dff527776bCRMmMH/+fNra2gBrJs8zzzyTKVOmMG/ePA4cOADArFmzuOeee5g2bRqjR4/m888/p7W1lfvuu49FixZRUFDAokWLjlrYZfz48RQXF3tUdm8LqgnX4Ls7fe3Bo1QAvHsvHNzo3d8cPAHO/V2vSXbs2MGrr77KE088wcknn8yLL77I8uXLWbJkCb/97W/Jzs5m8uTJvPnmmyxbtozrrrvu8GRsRUVFfPzxx2zZsoVTTz2V1157jYcffphLLrmEd955h/PPP5877riDt956i7S0NBYtWsSvfvUrnn76aQDa29tZtWoVS5cu5YEHHuDDDz88avrmhQsXHnfZ33zzTe/8O7oFXdCPd4QTFxmm1TtKhZDc3FwmTJgAQH5+PnPmzEFEmDBhAsXFxezevfvwpGmzZ8/m0KFD1NRYM/Kee+65hIeHM2HCBJxOJ/Pnzwc4fOy2bdvYtGkTZ599NgBOp5MhQ4YczvvSSy8FYMqUKRQXF3u97N4WdEEfrLt9DfpKBUAfd+S+EhkZefi9zWY7/Nlms9He3k5Y2NGhrmM65s5pw8PDD+/vONYYQ35+PitWrOg1b7vdTnt7e7dpOk/TDEdO1dxX2b0t6Or0wZqDR6t3lFIdzjjjDF544QUAPvnkE1JTU4mPj/fo2DFjxlBeXn446Le1tbF58+Zej+k65XJOTg5ff/01AF9//TW7du06ntPwiqAM+rpsolKqs4ULF1JYWMjEiRO59957ee655zw+NiIigsWLF3PPPfcwadIkCgoK+PLLL3s95qyzzmLLli2HG3Ivu+wyKisrKSgo4G9/+xujR48+0VM6bkE1tXKH//5kBw//axtbHpxHdERQ1mAp1W/o1Mr+p1Mrd5GpPXiUUqpbQRn0vxugpX31lVKqM4+CvojMF5FtIrJDRO7t5vtIEVnk/n6liOR0+X6oiNSLyM+9U+ze6Z2+Uv7V36qJg9mJ/lv3GfRFxA78FTgXyAOuEpG8LsluAqqMMSOBR4Dfd/n+EeBd/GRQXCR2m2jQV8oPHA4Hhw4d0sDvB8YYDh06hMPhOO7f8KSVcxqwwxizE0BEXgYuArZ0SnMRsND9fjHwqIiIMcaIyMXATqDhuEt5jMLsNgbHO3TZRKX8ICsri5KSEsrLywNdlJDgcDjIyso67uM9CfqZwN5On0uA6T2lMca0i0gNkCIiTcA9wNlAj1U7InILcAvA0KFDPS58bzISHTpASyk/CA8PJzc3N9DFUB7ypE5futnX9TmupzQPAI8YY+p7y8AY84QxZqoxZmpaWpoHReqbLqailFJH8+ROvwTI7vQ5C9jfQ5oSEQkDEoBKrCeCBSLyMJAIuESk2Rjz6AmXvA+ZiVEs3XgAp8tgt3V3TVJKqdDjSdBfDYwSkVxgH3AlcHWXNEuA64EVwAJgmbFadWZ2JBCRhUC9PwI+WHf6bU5DeV0LgxOOv9FDKaWCSZ/VO8aYduB24D1gK/CKMWaziDwoIhe6kz2FVYe/A/gZcFS3Tn/L1MVUlFLqKB7NUWCMWQos7bLvvk7vm4HL+/iNhcdRvuPWeV79KcOS/Jm1Ukr1W0E5Ihes3jugA7SUUqqzoA36cY5w4h1hGvSVUqqToA36oIupKKVUV0Ed9DMTo3TSNaWU6iSog35Goi6mopRSnQV10M9MiqKmqY36Fu+vM6mUUgNRUAf9jm6bB/RuXymlgCAP+pnubpslGvSVUgoI8qCfoYupKKXUEYI66A+KcxCmi6kopdRhQR307TZhcIKD/dptUymlgCAP+uAeoKUraCmlFBACQT9TR+UqpdRhQR/0MxIdHKxtxunSRZuVUip4gn57CxxYD821R+zOSIzC6TKU1Wm9vlJKBU/Q378WHj8D9qw4YnemdttUSqnDgifop421Xsu2HLG7I+iXaGOuUkoFUdCPSoT4TCjbesTuDA36Sil1WPAEfYBB446604+JDGNEWgxf7KgIUKGUUqr/CL6gX/4tOI+cVXNe/mBW7qqkqqE1QAVTSqn+IciCfh44W6Bq1xG7z8kfjNNl+OibsgAVTCml+ocgC/rjrNcuVTwTMxMYHO/g/c0HA1AopZTqP4Ir6KeOAQRKjwz6NptwTn46n20vp6nVGZiyKaVUPxBcQT8iGpJzj7rTB6tev7nNxafflgegYEop1T8EV9AHq16/S7dNgGm5ySREhWsVj1IqpAVn0K8sgrYjp10It9uYM24QH24tpc3pClDhlFIqsIIw6I8D44KKb4/6al7+YGqb21m5szIABVNKqcALwqCfZ712U8Vzxqg0HOE23tMqHqVUiAq+oJ8yAmzh3TbmRkXYOXN0Gu9vOYhLp1pWSoWg4Av69nBIHd3tnT5YVTyltS2sL6n2c8GUUirwgi/og3sOnu6D/pyx6dhtwvtbSv1cKKWUCrzgDfo1e45aUAUgITqcU4Yna72+UiokBWfQT8+3Xsu/6fbrefmD2VnewI6yOj8WSimlAi84g34Pc/B0OCdvMADvbdYqHqVUaAnOoJ8wFMJjeqzXH5zgYFJ2olbxKKVCjkdBX0Tmi8g2EdkhIvd2832kiCxyf79SRHLc+6eJyDr3tl5ELvFu8Xtgs8GgsT3e6QPMy09nQ0mNrp2rlAopfQZ9EbEDfwXOBfKAq0Qkr0uym4AqY8xI4BHg9+79m4CpxpgCYD7wuIiEeavwveqlBw9Y9fqAzsWjlAopntzpTwN2GGN2GmNagZeBi7qkuQh4zv1+MTBHRMQY02iM6VjGygH4b0TUoDxoKIf67mfVHJEWy6hBsby1fr/fiqSUUoHmSdDPBPZ2+lzi3tdtGneQrwFSAERkuohsBjYCP+50EThMRG4RkUIRKSwv99LUxx2NueU93+1fM30oa/dUs2Z3lXfyVEqpfs6ToC/d7Ot6x95jGmPMSmNMPnAy8EsRcRyV0JgnjDFTjTFT09LSPCiSB3qZg6fD5VOziXeE8eTnO72Tp1JK9XOeBP0SILvT5yyga53I4TTuOvsE4IipLI0xW4EGYPzxFvaYxKZDVFKvjbkxkWFcc8ow3tt8kD2HGv1SLKWUCiRPgv5qYJSI5IpIBHAlsKRLmiXA9e73C4BlxhjjPiYMQESGAWOAYq+UvC8iMCi/1zt9gBtOy8FuE57+Ylev6ZRSKhj0GfTddfC3A+8BW4FXjDGbReRBEbnQnewpIEVEdgA/Azq6dZ4OrBeRdcAbwK3GmApvn0SPOnrwmJ7bj9PjHXxvUgavFO6lprHNb0VTSqlA8Kj7pDFmKbC0y777Or1vBi7v5rjngedPsIzHb9A4aKmFmhJIzO4x2c2nD+f1r/fxwqrd3DprpB8LqJRS/hWcI3I7eNCYC5CXEc/pI1N57stiWtt1KUWlVPAK8qA/1nrtpTG3w80zcymtbeFt7bevlApiwR30o5IgLqPPO32AM0enMTo9lr9/vhPTSxuAUkoNZMEd9MHdmNv3nb6IcPPpw/nmYB3Ld/ivrVkppfwpNIJ++TZwOftMetHkDFJjI/n759p9UykVnII/6Kfng7MFKvsO5JFhdq4/dRiffVvOtoO6wIpSKvgEf9DvY0GVrq49ZRiOcJtOzaCUCkrBH/RTx4DYYN8aj5InxURwxdRs3li7jy37j15jVymlBrLgD/oR0TBiDmx81aN6fYCfnj2axOhw7nltA+1O7bevlAoewR/0ASZfC7X7YOfHHiVPjI5g4YX5bNxXo3PyKKWCSmgE/THnQlQyrP2Hx4ecP2EIc8el86cPvqW4osGHhVNKKf8JjaAfFgkTr4Bv3oHGyr7TY/Xb/83F4wm32fjl6xt1wJZSKiiERtAHmHwNOFutun0PDU5wcO95Y1mx8xCLVu/t+wCllOrnQifoD54AQwpg7bFN+nnVyUOZnpvMQ0u3Ulrb7KPCKaWUf4RO0AerQffgRjiw3uNDbDbhd5dNpLXdxX1vbfJh4ZRSyvdCK+hPWAD2yGNq0AXITY3hrrmjeW9zKe9uPOCjwimllO+FVtCPSoJx34MNr0DbsVXV/GhmLvkZ8fz6rc2U17X4qIBKKeVboRX0wariaa6Gbe8c02Fhdht/WDCJ+pY2bnm+kOY2zwZ6KaVUfxJ6QT/3TEjIPuYqHrBW2PrzFQWs3VPNLxZv0G6cSqkBJ/SCvs0GBddA0cdQfezdMOePH8Ld88fw9vr9/PnD7T4ooFJK+U7oBX2AgqsBA+tfOq7Df3LmCBZMyeI/P9rOW+v2ebdsSinlQ6EZ9JOGWdU8a/8BrmOfUE1E+O0lE5iWm8wvFm9gze4qHxRSKaW8LzSDPsDkH0D1bti9/LgOjwiz8fi1UxiS4OCW/ylkb2WjlwuolFLeF7pBf9wFEJkAa5497p9Iiong6RtOps3p4qbnVlPb3Oa98imllA+EbtAPj4Ip18HmN6D82+P+mRFpsTx27RR2ljdw/dOrqNPAr5Tqx0I36APMuAvCouDT353Qz5w2MpVHrz6JjSU1GviVUv1aaAf9mFSY/m+w6XUo9WwN3Z7MHz+YR6+ezIaSGm54ZjX1Le1eKqRSSnlPaAd9gNPugIhY+OTfT/in5o8fwn9dNZl1e6u5/ulVGviVUv2OBv3oZDj1Vti6BA5sOOGfO3fCEB51B/4bNPArpfoZDfoAp9wKjgT4+Lde+blzJ1h3/Gv3VnPjMxr4lVL9hwZ9gKhEOPUO+PZd2LfGKz953oQh/OXKyXy9p5rz//I5n35b7pXfVUqpE6FBv8P0f7OmXvbS3T7A+ROH8PxN07CJcP3Tq7jtxa919S2lVEBp0O/giIcZd8KOD2HPSq/97GkjUnn3zpn8dO5oPthSypz/+JRnvtiF06UzdCql/E+Dfmcn/wiiU+Hjh7z6s45wO3fOHcX7d53B5KGJPPD2Fi7663I27avxaj5KKdUXDfqdRcbC6T+FXZ9C8fHNydObnNQY/ueH03j06smU1bZwxeMrdLI2pZRfeRT0RWS+iGwTkR0icm8330eKyCL39ytFJMe9/2wRWSMiG92vs71bfB+Y+kOITYdlD4EPFkkRES6YmME/7zidtLhIbnh6FRtKqr2ej1JKdafPoC8iduCvwLlAHnCViOR1SXYTUGWMGQk8Avzevb8C+J4xZgJwPfC8twruMxHRcObdsOdL2Py6z7IZFO/gxR+dQkJ0OD94ahVb9tf6LC+llOrgyZ3+NGCHMWanMaYVeBm4qEuai4Dn3O8XA3NERIwxa40x+937NwMOEYn0RsF9asqNMHgivPcraKnzWTYZiVG89KNTiI6w84OnVrK91Hd5KaUUeBb0M4HO6wqWuPd1m8YY0w7UACld0lwGrDXGtHTNQERuEZFCESksL+8H/dltdjj/P6DuAHz6sE+zyk6O5oWbp2OzCVc/uZJdFQ0+zU8pFdo8CfrSzb6uld29phGRfKwqn3/rLgNjzBPGmKnGmKlpaWkeFMkPsqdBwbXw1X9D+TafZjU8LZYXb56O02W4+u9f6YIsSimf8STolwDZnT5nAft7SiMiYUACUOn+nAW8AVxnjCk60QL71dyFEBEDS3/uk0bdzkalx/GPm6bT2Ork+4+v0BG8Simf8CTorwZGiUiuiEQAVwJLuqRZgtVQC7AAWGaMMSKSCLwD/NIY84W3Cu03sWkw+9ew6zNrsRUfy8uI54WbpxMdYef6p1dx18trOVR/VG2YUkodtz6DvruO/nbgPWAr8IoxZrOIPCgiF7qTPQWkiMgO4GdAR7fO24GRwK9FZJ17G+T1s/ClqT/s1Khb7/PsxmcmsPTOmfyvOaN4Z+MB5v7pU15bU4Lx8ZOGUio0SH8LJlOnTjWFhYWBLsaR9q6Cp862pmk4+0G/ZfttaR33vraBr/dUM3NUKg9dPIGhKdF+y18pNXCIyBpjzNS+0umIXE90NOqu+KvPG3U7G50ex+Ifn8aDF+Wzdk815/z5U95cu89v+Sulgo8GfU/NXehu1P2Fzxt1O7PZhOtOzeGDn53BxKxE7lq0jgff3kKb0+W3MiilgocGfU/FpsGc+6x5ed75Gbj8G3SHJETxws3TuXFGDk9/sYtrn1xJhTbyKqWOkQb9YzH1JphxFxQ+DUtuB5fTr9mH223c/718HrliEuv2VvO9/1rO+r06b49SynMa9I+FiFXNM+uXsO4FeP1H4GzzezEumZzFaz85DZsIlz++glcK9/Z9kFJKoUH/2InArHth7gOw6TV49QZo9381y/jMBN6+43ROzkni7sUbuPGZVTo/v1KqTxr0j9fpd8H838M3/4RF10Kb/5dBTI6J4Lkbp3HvuWP5ek81F/zXcm59YQ07ynTiNqVU97Sf/okqfAb++VPIPQOuesnq4RMANU1tPLV8F099vpOmNicXT87krjmjtV+/UiHC0376GvS9Yd1L8NatkH0KXPMKRMYFrCiVDa08/mkRz60opt1pWDAlix+fOYKc1MBcjJRS/qFB3982vQav/QgyT4JrFkNUYkCLU1bbzF8/3sFLq/fS7nTxvUkZ3DprJGMGB+6CpJTyHQ36gbD1bXj1RkjPgx+8CdHJgS4RZXXNPPX5Lv7x1W4aWp2ck5fO7bNHMjErsBclpZR3adAPlG/fg0U/gNRRVuCP7R/rA1Q3tvLMF8U8+2UxNU1tnDUmjf97QR4j0mIDXTSllBdo0A+komXw0tWQOBSuXwJxgwNdosPqW9p5fsVu/vuTHTS1OrlxRg7/a84o4hzhgS6aUuoE6IRrgTRiNly7GGpK4Jlzrdd+IjYyjJ/MGsHHP5/FgilZPLl8F2f98VNeLdyLy9W/bgCUUt6nQd9Xck6HH7wBDRXw5Fw4sD7QJTpCamwkv7tsIktuO52hyVH8YvEGLvnbl6zZXRXooimlfEird3zt4CZ48QpoqoIFT8OY+YEu0VGMMby5bh//vvQbyupaGJMex0WTM7ioIJPMxKhAF08p5QGt0+9P6g5agf/gBpj/O5je7frwAVff0s4bX5fw5rr9h+/4p+Umc8nkTM4bP4SEaK33V6q/0qDf37Q2WP34t70D038M834LNnugS9WjPYcaeWvdPt5Yt4+d5Q1E2G18/+QsfjJrpN79K9UPadDvj1xO+OA+WPEojJ4Plz0Fkf27y6Qxhk37anlx1R4Wr7Fm81wwJYtbZ40kO1mneFCqv9Cg35+tftJagStjstWX3xEf6BJ5ZH91E499WsTLq/biMoZLT8rktrNGMixFp3hQKtA06Pd337wDr1wH2dOtaRsiBs5d88GaZh7/rIgXV+6h3WWYPXYQC6ZkMXvsIMLt2iFMqUDQoD8QbFwMr90MI+fAlS9CWGSgS3RMyuqaeXp5Ma99XUJ5XQspMRFcVJDJgilZ5GUMjKcXpYKFBv2B4uv/gSV3wNgL4PLnwB4W6BIds3ani8+2l7N4TQkfbimj1ekib0g8FxVkcHZeOsN1qgelfE6D/kDy1d/gX/fCxCvh4r+BbeBWkVQ1tPL2hv0sXlPChhJrJa8RaTHMzUvnnLx0CrKTsNskwKVUKvho0B9oPvsDLPsNTP0hnP8na1nGAa6kqpGPtpbxwZZSvtp5iHaXITU2grPzBrNgShYnDU1EguA8leoPNOgPNMbAhwvhiz9b/fjnLoTw4OkPX9PUxifbrAvAR1vLaGpzMiIthsunZnPp5EwGxTsCXUSlBjQN+gORMVY1z8rHIG4IzPzfcNJ1A66Bty91zW0s3XiAVwpLWLO7CrtNmDU6jQVTsjhr7CAc4f130JpS/ZUG/YGseDksewj2fAkJ2XDm3TDpKrAH3zQIReX1vFpYwutfl1BW10JMhJ3Z49I5b/xgZo0ZRFSEXgCU8oQG/YHOGGte/o8fgn1rICkXZv0SJlw+oBt6e9LudLFi5yGWbjzIe5sPUtnQSlS4ndljB3HuhMGcOTpN5/xXqhca9IOFMdZqXB//Bg5uhCEFcO7DMHR6oEvmM+1OF6t2VbJ00wH+tekgFfWthNuFabnJzB6bzpyxg3Shd6W60KAfbFwu2LQYPrgf6vZbd/xzH4CEzECXzKecLkNhcSXLtpWxbGsZ28vqARieFsPsMYM4c0waJ+ckazuACnka9INVawMs/zN8+RcQG5z+UzjtjqDq6dObvZWNLPumjI++KeOrokO0Ol1Ehtk4OSeZ00elcvrIVPKGxGPTsQAqxGjQD3ZVu60ZO7e8aTX2zv510Nb396SxtZ2VuypZvr2C5dsr2FZaB0ByTASnjUjh9JGpzBiZqrOBqpCgQT9UFC+3unke3AiD8mHOfTB6XlAM7jpWZbXNLN9hXQCW76igrK4FgOzkKE4fmcppI6yLQHI0EdtmAAAUSElEQVRMRIBLqpT3adAPJS4XbH7dGtFbtQuyT7EGdw07NdAlCxhjDEXl9SzfXsEXRYf4qugQdS3t2AROzklm/vjBzMsfTIYuCKOChFeDvojMB/4TsANPGmN+1+X7SOB/gCnAIeAKY0yxiKQAi4GTgWeNMbf3lZcG/RPgbLMmcPv091BfCqPmwam3wdBTISy0727bnS427Kvhk2/K+Nfmg3xbajUIT8pOZF5+OnPHpTM0OVobhNWA5bWgLyJ24FvgbKAEWA1cZYzZ0inNrcBEY8yPReRK4BJjzBUiEgNMBsYD4zXo+0lrI6x6HJY/As01EBEHI86yqn1Gng1x6YEuYcAVldfz3uaDvLfpIOvdE8MBxDvCSI93kB7vYFB8JIPjHZwyPIVTR6ToWgGqX/Nm0D8VWGiMmef+/EsAY8y/d0rznjvNChEJAw4Cacb94yJyAzBVg76ftTbAzk+sfv7bP7C6eoLV13/0PBh1DmScFFKNv93ZV93EiqJDlNY2U1bbTGltC6V1zZTVtlBa20y7yxDvCGPOuHTm5VsDxXSksOpvPA36nkzengns7fS5BOg6MuhwGmNMu4jUAClAhYeFvQW4BWDo0KGeHKI8EREDY8+3NmOsxt7t71vbZ3+wqoGiU2HkXBh9DoyYDVFJgS6132UmRrFgSla33zW3Ofl8ewX/2nSQj74p5Y21+3CE2zhjVBozRqYyPjOecUPiiY4YeOsgqNDkyV9qd91Auj4eeJKmR8aYJ4AnwLrT9/Q4dQxEYMhEazvj59BYCTs+gu3vWduGl0HskHsGnPt7SBsT6BL3C45wO2fnpXN2Xjpt7pHC720+yPubS3l/SykANoHhabGMz4hnfGYCEzITmJCVoBcC1S958ldZAmR3+pwF7O8hTYm7eicBqPRKCZVvRCfDxMutzeW05vf59j0ofBoem2l1/Tzl1pCv+uks3G5jhrvv/wMX5nOwtplN+2rZtK+GzftrWLHzEG+us/7XsNuEMelxFAxNpCA7kcnZiYxIi9VBYyrgPKnTD8NqyJ0D7MNqyL3aGLO5U5rbgAmdGnIvNcZ8v9P3N6B1+gNDXSm8fSd8+y4MmwEX/RWScwNdqgGjrK6ZDXtrWLe3mnV7q1m/t5q6lnYAYiPDGDckjnFD4skbEk9eRjyj0+O0x5DyCm932TwP+DNWl82njTEPiciDQKExZomIOIDnsXrqVAJXGmN2uo8tBuKBCKAaOKdzz5+uNOj3A8bA+pfg3Xusp4B5v4EpN4bkgK8T5XIZdlbUs3ZPNRv31bD1QC1bD9RR774Q2ARGpMUyKTuRSe4ngjGD47SnkDpmOjhLnbiaEnjrNqsH0PCzrDl+cs8ckIu39ycul2FvVSNb9tey5YBVPbS+pIbKhlYAIsNsTMhMYFJ2IhOzEsjPSCA3NUbXFla90qCvvMMYKHwKPnwAWmohOgXGXQjjL7Wqf2xaNeENxhhKqppY664SWre3mk37amhpdwEQHWEnb4jVUDw+M4HR6bHkpMYQr2sMKDcN+sq72pphxwew6XX49l/Q1gix6ZB3kfUUkHUyxKYFupRBpc3poqi8/ojG4s37a2lsdR5OkxITQU5qDDkpMeSmRjMqPY6C7ETSdc3hkKNBX/lOa4PV13/T69Zre7O1PynHCv5ZJ0PWVEifEPLTP3ib02XYVdFAUXk9xRUN7HJvxYcaKK1tOZwuI8HB5KFJVs+hoYmMz0zQBuMgp0Ff+UdbE+xfByWrv9vqDljfhUdbF4Cc062qoMwpEK53oL7S2NrONwfrWLunmrV7qli3t5qSqibAajDOSopmRFoMI9JiGTEolhFpsWQlRVHZ0EpJVSMlVU2Ht9LaZkalx3Lu+CHMHJWqF4wBQIO+CpyafVCyCnavgN1fQOlmwIA90noCyJlpjf7NnKKNwj5WXtfCur1Wz6Gi8nqKyurZVdFwuK2gq9jIMLKSokiLi2T93mpqm9uJibBz1thBnDt+CLPGpBETqf/N+iMN+qr/aKyEPV9ZF4Di5XBgPWDAkWD1Bho5B0bMgcTsPn9KnTiXy7Cvuomi8npKqppIjY0kKymK7KRo4qPCEHfX3NZ2F1/tPMS7mw7y/uaDHGpoJTLMxvThKUwdlsSUYVb1kV4E+gcN+qr/aqy0uoEWfQQ7ln03EVzaODjpOii4KiTnAOrPnC7D6uJK/rXpIF/tPMS20jqMsUYejxsSx9RhyYwdHEecI5yYSDuxkWHEOsKIiQgjKSaCWL0w+JwGfTUwGAPl26wLwOY3rDaBsChreoiTb4YhkwJdQtWNmqY21u6pYs3uKgqLrfaDpjZnj+lzU2OYlGWNPSjITiQvI57IMG0n8CYN+mpgOrAeVj8JG16F9iarIXjKDZAyCiLjwBFvvUbE6hiBfqTN6aKsroWGlnbqmttpaGmn3r2V17UcHnvQsYRluF0YNySeUYPiGJ4Wc7iBeWhKtF4MjpMGfTWwNVVbU0GsfhIO7eg+TUQsZE+D8Qtg3AVWG4Hq1w7UNLkvADVsKKmmqLz+iK6mNoHs5GiGp8YwPC2W3NQY90UhlkFxkYfbG9TRNOir4GCMdfffWAEtddBca7221EHjIWucQPVuq2fQqLNhwgIYPR/Cde3bgaKuuY1dFQ3sLLfGH+wsb2BnRQO7Kuppbvuul1FMhJ2spGgGxUcyKM7hfrXep8VFkhgdTkKUtYViF1NvLqKiVOCIQEZBz98bY00LvXGxtTj8N/90PwFMh6RhkJANiUOtLSHbGkWs00X3K3GOcCZmJTIxK/GI/S6X4WBtMzvLrQtAUXkD+6qbKKtroaisgrK6Ftpd3d+0OsJtJESFkxwTyfC0GMakxzE6PZZR6XEMS44mLIQntNM7fRU8XE6rS+im16yng5q91tNAZ5EJMHK2tVTkyLN16ogBzOUyVDW2UlbXQkV9CzVNbVQ3tlHT5N4a2yivb2FHWT17qxrpCHURYTaGp8aQFB1BdISdqAg70RF2oiPCiIqwMzjewfA0q3ppSLxjwKyBoHf6KvTY7DD8TGvr0FJvzRZavceqBjqwzlovePMb1vcZJ7kvAHMgdZR2FR1AbDYhJTaSlNjIPtM2trazo6yeb0vr2V5ax46yemqb2zhY20ZTq5PGVieNre00tjqPeHpwhNvISYk5PHo5OSaCpJgIkqMjSIoJJyk6gtS4yAE18Z3e6avQ43LBwQ1W8N/+vtVNtGN1T0eCNYdQx5Y4FMIcIDZArFdxvw6ZZF0oVNAwxlBe10JReQM7K9ztC+X17Kxo4EB1M63O7kcyJ0SFMzQ5mqHJ0WQlRzE0OZrMxCjiHGE4wu1EhVtPFB2vvuihpA25Snmq4RDsWQFVu6Cq2L3ttp4MnK29H5s5BSZdBfmXQkyKP0qrAsQYQ2Ork8qGVqoaWw+/ltW2sLeqkb2VTeyttOYw6uni0CE1NsI9M2oMuWkx5KZYrzkpMcfdCK1BX6kT5XJBfak78BswLg5XDDtbrYXl178MpRvBFm5VE0260nrVieVCltNlKK1tZn91Ew2tTppanTS3OWlqs14bWtopqWo6PENqx9gFgLPz0vn7dX3G7W5pnb5SJ8pmg/ghPX8/aBycdjsc3GgF/42vwrZ3QOxW1VDaGKv6J3WM9T4+w7pYtDVbA886XsF6YtBxBkHBbhMyEqPISPSs23B9S/vhabKTon0/Fbne6SvlLc522PUJ7FkJFdug/FtrYJmrre9jxW4NNBsxx2pUHlKgXUvVMdHqHaX6A2e71TZQvs1aZyDMYVX9hEVBWKQ1iKy9BYo/hx0fumcgxVqWcvhZMGQiJA+H5BGQnKuDzlSPNOgrNRDVl0PRMmsCup2fWG0KncVnWheB2EHWILSOeYgiYyEiBqJTraql5FzrOxUytE5fqYEoNg0mXWFtYM1BVLnzu+1QEVQWwf611hiE1nprveLuRKdAUq51EUgZAenjYfAESBymVUchTIO+Uv1ZVCJknmRtPXE5rXWLW+uhvszd5bRT99OS1dYUFcbdjTAy/rsLQHq+1cAck2Y9PcSkgX3gDDRSx06DvlIDnc1uTTntiLcCeHdzFbU1QdlWq6dRx7buBetC0VVUEsQMsgamJQ8/cksapheFAU6DvlKhIDzq6CcGlwtq9kBdKTSUWU8JDeXWa32pNXXFnhVHXhjEbj0NRCVZW3Sy9TQSlWTNaxQWYTVWh0Var/YI68kidSQkDNVqpX5Ag75Socpm+266iZ4YAw0VndoViqwLQlMVNFZB5S7rfVPVd2MOehIeDamjIW2sNW4hbYzV8OyIty4MjnirUVrnzPcpDfpKqZ6JWI3LsWkwdHrvaZ1tVvfT9hZob7Y2Z6vVGF2xzeq2Wv4N7PoMNrzcQ342q9dRdIr1RNGxdbQ3RMQAnS4KHRcIZ+t3F5+OrbHS2p8xGYbNsDadKkODvlLKS+zh1hYZe/R3w0498nNzjTVwranKvTBObafXGmtK7IZyq7fSnq/cU2R70L3cFgZRyd9VP4kN1jwLKx+zvk8bBzkzYOip7kbrCKtKyt5pi0oER2LQPnFo0FdK+Z8jwZp6wlPOdivwH9E9tdNFwBZmBfnuqofaW6wursXLYfcXsM69DGdv7JHWgjuxg6zXuHTrQhAW6b64RbrfR1gXufgsSMiCuMH9fu1mDfpKqf7PHmYF3uMRFglDT7E2fm5VQ5VtsZbcbG+xPjtbv9saK612i44G7apdsPcr6wnE1d57XrYwqwdVwlDrNTLWassIj3KPxna/j062ekh1VFtFxvntyUKDvlIqtNjDrbUQjofL+d3Fob0VnC3WxaOmxFqprXrvd+/3rrSeTNqarFfTy3TLYQ4r+OddBPMeOr6yeUiDvlJKecpmB1vU0XMgDRrX+3HGuGdYbbIG0jVVHtlFtqHMmoIjPtN3ZXfToK+UUr4m4h67EGk1FCf4Prj3REdKKKVUCNGgr5RSIcSjoC8i80Vkm4jsEJF7u/k+UkQWub9fKSI5nb77pXv/NhGZ572iK6WUOlZ9Bn0RsQN/Bc4F8oCrRCSvS7KbgCpjzEjgEeD37mPzgCuBfGA+8N/u31NKKRUAntzpTwN2GGN2GmNagZeBi7qkuQh4zv1+MTBHRMS9/2VjTIsxZheww/17SimlAsCToJ8J7O30ucS9r9s0xph2oAZI8fBYROQWESkUkcLy8nLPS6+UUuqYeBL0uxsm1nUSjJ7SeHIsxpgnjDFTjTFT09LSPCiSUkqp4+FJ0C8Bsjt9zgL295RGRMKABKDSw2OVUkr5SZ8Lo7uD+LfAHGAfsBq42hizuVOa24AJxpgfi8iVwKXGmO+LSD7wIlY9fgbwETDKGOPsJb9yYPcJnFMqUHECx58IzVvz1rw170DlPcwY02dVSZ8jco0x7SJyO/AeYAeeNsZsFpEHgUJjzBLgKeB5EdmBdYd/pfvYzSLyCrAFaAdu6y3gu485ofodESn0ZEV4X9C8NW/NW/Pu73l7NA2DMWYpsLTLvvs6vW8GLu/h2IcA384gpJRSyiM6IlcppUJIMAb9JzRvzVvz1rw17+712ZCrlFIqeATjnb5SSqkeaNBXSqkQEjRBv6+ZQH2cd7GIbBSRdSJS6OO8nhaRMhHZ1Glfsoh8ICLb3a9Jfsx7oYjsc5/7OhE5z0d5Z4vIxyKyVUQ2i8id7v0+P/de8vb5uYuIQ0RWich6d94PuPfnume03e6e4TbCj3k/KyK7Op13gbfz7lQGu4isFZF/uj/7/Lx7yduf531UTPHa37oxZsBvWOMHioDhQASwHsjzY/7FQKqf8joDOAnY1Gnfw8C97vf3Ar/3Y94LgZ/74byHACe538dhDRjM88e595K3z88dayqTWPf7cGAlcArwCnCle/9jwE/8mPezwAJf/zd35/szrAGe/3R/9vl595K3P8/7qJjirb/1YLnT92Qm0KBgjPkMawBcZ51nOX0OuNiPefuFMeaAMeZr9/s6YCvW5H0+P/de8vY5Y6l3fwx3bwaYjTWjLfjuvHvK2y9EJAs4H3jS/Vnww3l3l3c/4ZW/9WAJ+h7N5ulDBnhfRNaIyC1+zLdDujHmAFgBChjk5/xvF5EN7uofn1QtdSbWIj2Tse48/XruXfIGP5y7u5phHVAGfID1VFttrBltwYd/713zNsZ0nPdD7vN+REQifZE38GfgbsDl/pyCn867m7w7+OO8ofuY4pW/9WAJ+h7N5ulDM4wxJ2EtNHObiJzhx7wD7W/ACKAAOAD8hy8zE5FY4DXgLmNMrS/z8iBvv5y7McZpjCnAmrBwGjCuu2T+yFtExgO/BMYCJwPJwD3ezldELgDKjDFrOu/uroh+yhv8cN6d+CymBEvQD+hsnsaY/e7XMuAN/L9QTKmIDAFwv5b5K2NjTKk7MLiAv+PDcxeRcKyg+4Ix5nX3br+ce3d5+/Pc3flVA59g1asnijUZIvjh771T3vPd1V3GGNMCPINvznsGcKGIFGNV187Guvv2x3kflbeI/MNP5w30GFO88rceLEF/NTDK3bIfgTXh2xJ/ZCwiMSIS1/EeOAfY1PtRXrcEuN79/nrgLX9l3PFH6HYJPjp3d33uU8BWY8yfOn3l83PvKW9/nLuIpIlIovt9FDAXq03hY2CBO5mvzru7vL/pFHgEq17Z6+dtjPmlMSbLGJOD9f/zMmPMNfjhvHvI+1p/nLf793uKKd75W/dHS7Q/NuA8rF4VRcCv/JjvcKzeQuuBzb7OG3gJqyqhDesJ5yasus6PgO3u12Q/5v08sBHY4P6jHOKjvE/HepTfAKxzb+f549x7ydvn5w5MBNa689gE3Nfp724V1hKkrwKRfsx7mfu8NwH/wN3Dx4d/87P4rgeNz8+7l7z9ct49xRRv/a3rNAxKKRVCgqV6RymllAc06CulVAjRoK+UUiFEg75SSoUQDfpKKRVCNOgrpVQI0aCvlFIh5P8DNaPaMzmByDgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, epochs + 1, 5)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(model.loss_train_history, label = 'default')\n",
    "plt.plot(model_momentum.loss_train_history, label = 'momentum')\n",
    "plt.xticks(x)\n",
    "plt.title('Train loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.plot(model.loss_val_history, label = 'default')\n",
    "plt.plot(model_momentum.loss_val_history, label = 'momentum')\n",
    "plt.xticks(x)\n",
    "plt.title('Validate loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: Метод стохастического градиентного спуска осуществляет коррекцию весов слоёв нейронной сети и, таким образом, обеспечивает обучение нейронной сети. При использовании информации о градиентах с предыдущих шагов обучения надежность оценки градиента возрастает, и нейронная сеть достигает большей точности (меньшего значения функции потерь) за то же число шагов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
